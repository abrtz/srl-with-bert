{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all Required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from utils import read_data_as_sentence,map_labels_in_dataframe,tokenize_and_align_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data files and save them as dataframe.\\\n",
    "\\\n",
    "Use `read_data_as_sentence` function from `utils.py` to read files as Dataframe.\\\n",
    "\n",
    "`read_data_as_sentence` need two input:\n",
    "1. path of conllu file.\n",
    "2. path for save datarame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_data_as_sentence('data/en_ewt-up-train.conllu', 'data/en_ewt-up-train.preprocessed.csv')\n",
    "dev_data = read_data_as_sentence('data/en_ewt-up-dev.conllu', 'data/en_ewt-up-dev.preprocessed.csv')\n",
    "test_data = read_data_as_sentence('data/en_ewt-up-test.conllu', 'data/en_ewt-up-test.preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show 20 input form and their gold list in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'if', 'Google', 'Morphed', 'Into', 'GoogleOS', '?', '[SEP]', 'morph']\t:\t['_', '_', 'ARG1', '_', '_', 'ARG2', '_', None, '_']\n",
      "['What', 'if', 'Google', 'expanded', 'on', 'its', 'search', '-', 'engine', '(', 'and', 'now', 'e-mail', ')', 'wares', 'into', 'a', 'full', '-', 'fledged', 'operating', 'system', '?', '[SEP]', 'expand']\t:\t['_', '_', 'ARG0', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'ARG1', '_', '_', '_', '_', '_', '_', 'ARG4', '_', None, '_']\n",
      "['(', 'And', ',', 'by', 'the', 'way', ',', 'is', 'anybody', 'else', 'just', 'a', 'little', 'nostalgic', 'for', 'the', 'days', 'when', 'that', 'was', 'a', 'good', 'thing', '?', ')', '[SEP]', 'way']\t:\t['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', None, '_']\n",
      "['(', 'And', ',', 'by', 'the', 'way', ',', 'is', 'anybody', 'else', 'just', 'a', 'little', 'nostalgic', 'for', 'the', 'days', 'when', 'that', 'was', 'a', 'good', 'thing', '?', ')', '[SEP]', 'be']\t:\t['_', '_', '_', '_', '_', 'ARGM-DIS', '_', '_', 'ARG1', '_', '_', '_', '_', 'ARG2', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', None, '_']\n",
      "['(', 'And', ',', 'by', 'the', 'way', ',', 'is', 'anybody', 'else', 'just', 'a', 'little', 'nostalgic', 'for', 'the', 'days', 'when', 'that', 'was', 'a', 'good', 'thing', '?', ')', '[SEP]', 'be']\t:\t['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'ARGM-TMP', 'R-ARGM-TMP', 'ARG1', '_', '_', '_', 'ARG2', '_', '_', None, '_']\n",
      "['This', 'BuzzMachine', 'post', 'argues', 'that', 'Google', \"'s\", 'rush', 'toward', 'ubiquity', 'might', 'backfire', '--', 'which', 'we', \"'ve\", 'all', 'heard', 'before', ',', 'but', 'it', \"'s\", 'particularly', 'well', '-', 'put', 'in', 'this', 'post', '.', '[SEP]', 'post']\t:\t['_', 'ARG2', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', None, '_']\n",
      "['This', 'BuzzMachine', 'post', 'argues', 'that', 'Google', \"'s\", 'rush', 'toward', 'ubiquity', 'might', 'backfire', '--', 'which', 'we', \"'ve\", 'all', 'heard', 'before', ',', 'but', 'it', \"'s\", 'particularly', 'well', '-', 'put', 'in', 'this', 'post', '.', '[SEP]', 'argue']\t:\t['_', '_', 'ARG0', '_', '_', '_', '_', '_', '_', '_', '_', 'ARG1', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', None, '_']\n",
      "['This', 'BuzzMachine', 'post', 'argues', 'that', 'Google', \"'s\", 'rush', 'toward', 'ubiquity', 'might', 'backfire', '--', 'which', 'we', \"'ve\", 'all', 'heard', 'before', ',', 'but', 'it', \"'s\", 'particularly', 'well', '-', 'put', 'in', 'this', 'post', '.', '[SEP]', 'rush']\t:\t['_', '_', '_', '_', '_', 'ARG1', '_', '_', '_', 'ARG2', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', None, '_']\n",
      "['This', 'BuzzMachine', 'post', 'argues', 'that', 'Google', \"'s\", 'rush', 'toward', 'ubiquity', 'might', 'backfire', '--', 'which', 'we', \"'ve\", 'all', 'heard', 'before', ',', 'but', 'it', \"'s\", 'particularly', 'well', '-', 'put', 'in', 'this', 'post', '.', '[SEP]', 'backfire']\t:\t['_', '_', '_', '_', '_', '_', '_', 'ARG1', '_', '_', 'ARGM-MOD', '_', '_', '_', '_', '_', '_', 'ARGM-ADV', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', None, '_']\n",
      "['This', 'BuzzMachine', 'post', 'argues', 'that', 'Google', \"'s\", 'rush', 'toward', 'ubiquity', 'might', 'backfire', '--', 'which', 'we', \"'ve\", 'all', 'heard', 'before', ',', 'but', 'it', \"'s\", 'particularly', 'well', '-', 'put', 'in', 'this', 'post', '.', '[SEP]', 'have']\t:\t['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', None, '_']\n",
      "['This', 'BuzzMachine', 'post', 'argues', 'that', 'Google', \"'s\", 'rush', 'toward', 'ubiquity', 'might', 'backfire', '--', 'which', 'we', \"'ve\", 'all', 'heard', 'before', ',', 'but', 'it', \"'s\", 'particularly', 'well', '-', 'put', 'in', 'this', 'post', '.', '[SEP]', 'hear']\t:\t['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'ARG1', 'ARG0', '_', 'ARGM-ADV', '_', 'ARGM-TMP', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', None, '_']\n",
      "['This', 'BuzzMachine', 'post', 'argues', 'that', 'Google', \"'s\", 'rush', 'toward', 'ubiquity', 'might', 'backfire', '--', 'which', 'we', \"'ve\", 'all', 'heard', 'before', ',', 'but', 'it', \"'s\", 'particularly', 'well', '-', 'put', 'in', 'this', 'post', '.', '[SEP]', 'be']\t:\t['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'ARG1', '_', 'ARGM-ADV', '_', '_', 'ARG2', '_', '_', 'ARGM-LOC', '_', None, '_']\n",
      "['This', 'BuzzMachine', 'post', 'argues', 'that', 'Google', \"'s\", 'rush', 'toward', 'ubiquity', 'might', 'backfire', '--', 'which', 'we', \"'ve\", 'all', 'heard', 'before', ',', 'but', 'it', \"'s\", 'particularly', 'well', '-', 'put', 'in', 'this', 'post', '.', '[SEP]', 'post']\t:\t['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', None, '_']\n",
      "['Google', 'is', 'a', 'nice', 'search', 'engine', '.', '[SEP]', 'be']\t:\t['ARG1', '_', '_', '_', '_', 'ARG2', '_', None, '_']\n",
      "['Does', 'anybody', 'use', 'it', 'for', 'anything', 'else', '?', '[SEP]', 'do']\t:\t['_', '_', '_', '_', '_', '_', '_', '_', None, '_']\n",
      "['Does', 'anybody', 'use', 'it', 'for', 'anything', 'else', '?', '[SEP]', 'use']\t:\t['_', 'ARG0', '_', 'ARG1', '_', 'ARG2', '_', '_', None, '_']\n",
      "['They', 'own', 'blogger', ',', 'of', 'course', '.', '[SEP]', 'own']\t:\t['ARG0', '_', 'ARG1', '_', 'ARGM-ADV', '_', '_', None, '_']\n",
      "['Is', 'that', 'a', 'money', 'maker', '?', '[SEP]', 'be']\t:\t['_', 'ARG1', '_', '_', 'ARG2', '_', None, '_']\n",
      "['I', \"'m\", 'staying', 'away', 'from', 'the', 'stock', '.', '[SEP]', 'be']\t:\t['_', '_', '_', '_', '_', '_', '_', '_', None, '_']\n",
      "['I', \"'m\", 'staying', 'away', 'from', 'the', 'stock', '.', '[SEP]', 'stay']\t:\t['ARG1', '_', '_', 'ARG3', '_', '_', '_', '_', None, '_']\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(f\"{test_data.input_form[i]}\\t:\\t{test_data.argument[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head of test data after process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_form</th>\n",
       "      <th>argument</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[What, if, Google, Morphed, Into, GoogleOS, ?,...</td>\n",
       "      <td>[_, _, ARG1, _, _, ARG2, _, None, _]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[What, if, Google, expanded, on, its, search, ...</td>\n",
       "      <td>[_, _, ARG0, _, _, _, _, _, _, _, _, _, _, _, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(, And, ,, by, the, way, ,, is, anybody, else...</td>\n",
       "      <td>[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(, And, ,, by, the, way, ,, is, anybody, else...</td>\n",
       "      <td>[_, _, _, _, _, ARGM-DIS, _, _, ARG1, _, _, _,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(, And, ,, by, the, way, ,, is, anybody, else...</td>\n",
       "      <td>[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_form  \\\n",
       "0  [What, if, Google, Morphed, Into, GoogleOS, ?,...   \n",
       "1  [What, if, Google, expanded, on, its, search, ...   \n",
       "2  [(, And, ,, by, the, way, ,, is, anybody, else...   \n",
       "3  [(, And, ,, by, the, way, ,, is, anybody, else...   \n",
       "4  [(, And, ,, by, the, way, ,, is, anybody, else...   \n",
       "\n",
       "                                            argument  \n",
       "0               [_, _, ARG1, _, _, ARG2, _, None, _]  \n",
       "1  [_, _, ARG0, _, _, _, _, _, _, _, _, _, _, _, ...  \n",
       "2  [_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, ...  \n",
       "3  [_, _, _, _, _, ARGM-DIS, _, _, ARG1, _, _, _,...  \n",
       "4  [_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the sentence representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'if', 'Google', 'expanded', 'on', 'its', 'search', '-', 'engine', '(', 'and', 'now', 'e-mail', ')', 'wares', 'into', 'a', 'full', '-', 'fledged', 'operating', 'system', '?', '[SEP]', 'expand']\n"
     ]
    }
   ],
   "source": [
    "example = test_data['input_form'][1]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence contains the [SEP] special token followed by the predicate. Therefore, the parameter `add_special_tokens` is set to True so that the index is converted to 102 accordingly and is not treated as another word. \\\n",
    "In addition, the sentence is already split into tokens, to the parameter `is_split_into_words` is also set to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2054, 2065, 8224, 4423, 2006, 2049, 3945, 1011, 3194, 1006, 1998, 2085, 1041, 1011, 5653, 1007, 16283, 2015, 2046, 1037, 2440, 1011, 26712, 4082, 2291, 1029, 102, 7818, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(example,add_special_tokens=True,is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'what', 'if', 'google', 'expanded', 'on', 'its', 'search', '-', 'engine', '(', 'and', 'now', 'e', '-', 'mail', ')', 'ware', '##s', 'into', 'a', 'full', '-', 'fledged', 'operating', 'system', '?', '[SEP]', 'expand', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(example,add_special_tokens=True,is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing input and preparing the labels for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the labels in the df to numerical values for the language model with `map_labels_in_dataframe` function. \\\n",
    "Add a new column to the df matching the arguments to label numbers. 0 stands for '_' (no argument) and the rest of the arguments are alphabetically ordered. \\\n",
    "*None* label will be mapped to the *[SEP]* token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = map_labels_in_dataframe(train_data)\n",
    "dev_data = map_labels_in_dataframe(dev_data)\n",
    "test_data = map_labels_in_dataframe(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the head to confirm the labels were correctly converted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_form</th>\n",
       "      <th>argument</th>\n",
       "      <th>mapped_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[What, if, Google, Morphed, Into, GoogleOS, ?,...</td>\n",
       "      <td>[_, _, ARG1, _, _, ARG2, _, None, _]</td>\n",
       "      <td>[0, 0, 2, 0, 0, 4, 0, None, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[What, if, Google, expanded, on, its, search, ...</td>\n",
       "      <td>[_, _, ARG0, _, _, _, _, _, _, _, _, _, _, _, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(, And, ,, by, the, way, ,, is, anybody, else...</td>\n",
       "      <td>[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(, And, ,, by, the, way, ,, is, anybody, else...</td>\n",
       "      <td>[_, _, _, _, _, ARGM-DIS, _, _, ARG1, _, _, _,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 15, 0, 0, 2, 0, 0, 0, 0, 4, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(, And, ,, by, the, way, ,, is, anybody, else...</td>\n",
       "      <td>[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_form  \\\n",
       "0  [What, if, Google, Morphed, Into, GoogleOS, ?,...   \n",
       "1  [What, if, Google, expanded, on, its, search, ...   \n",
       "2  [(, And, ,, by, the, way, ,, is, anybody, else...   \n",
       "3  [(, And, ,, by, the, way, ,, is, anybody, else...   \n",
       "4  [(, And, ,, by, the, way, ,, is, anybody, else...   \n",
       "\n",
       "                                            argument  \\\n",
       "0               [_, _, ARG1, _, _, ARG2, _, None, _]   \n",
       "1  [_, _, ARG0, _, _, _, _, _, _, _, _, _, _, _, ...   \n",
       "2  [_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, ...   \n",
       "3  [_, _, _, _, _, ARGM-DIS, _, _, ARG1, _, _, _,...   \n",
       "4  [_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, ...   \n",
       "\n",
       "                                       mapped_labels  \n",
       "0                     [0, 0, 2, 0, 0, 4, 0, None, 0]  \n",
       "1  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 15, 0, 0, 2, 0, 0, 0, 0, 4, 0,...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `tokenize_and_align_labels` function to tokenize train, test, and dev dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = tokenize_and_align_labels(tokenizer, test_data, label_all_tokens=True)\n",
    "tokenized_train = tokenize_and_align_labels(tokenizer, train_data, label_all_tokens=True)\n",
    "tokenized_dev = tokenize_and_align_labels(tokenizer, dev_data, label_all_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input for the model has the corresponding special token [CLS] followed by the tokenized sentence, the special token [SEP], the predicate and the final [SEP] token./\n",
    "The numerical labels to be fed to the model correspond to the tokenized sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'what', 'if', 'google', 'mor', '##ph', '##ed', 'into', 'google', '##os', '?', '[SEP]', 'mor', '##ph', '[SEP]']\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[101, 2054, 2065, 8224, 22822, 8458, 2098, 2046, 8224, 2891, 1029, 102, 22822, 8458, 102]\n",
      "[-100, 0, 0, 2, 0, 0, 0, 0, 4, 4, 0, -100, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokenized_test[\"input_ids\"][0]))\n",
    "print(tokenized_test[\"attention_mask\"][0])\n",
    "print(tokenized_test[\"input_ids\"][0])\n",
    "print(tokenized_test[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
