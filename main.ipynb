{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmZ_eppHYdEy"
   },
   "source": [
    "# Semantic role labeling with BERT\n",
    "\n",
    "In this notebook, you'll perform semantic role labeling with BERT, using the [English Universal Propbank 1.0 datasets](https://github.com/UniversalPropositions/UP-1.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XJQ6KZ5YdE1"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Install libraries\n",
    "\n",
    "If you run this notebook in [Google colab](https://colab.research.google.com), make sure to install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jbVPypolapSm",
    "outputId": "15724fc3-54a5-4bb5-b183-ee594d92bda7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Collecting accelerate==0.21.0\n",
      "  Using cached accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (2.1.0+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.27.2\n",
      "    Uninstalling accelerate-0.27.2:\n",
      "      Successfully uninstalled accelerate-0.27.2\n",
      "Successfully installed accelerate-0.21.0\n",
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.21.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.21.0\n",
      "    Uninstalling accelerate-0.21.0:\n",
      "      Successfully uninstalled accelerate-0.21.0\n",
      "Successfully installed accelerate-0.27.2\n",
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    runs_in_colab =  True\n",
    "except ImportError:\n",
    "    runs_in_colab = False\n",
    "\n",
    "if runs_in_colab:\n",
    "    !pip install datasets\n",
    "    !pip install accelerate==0.21.0\n",
    "    !pip install transformers[torch]\n",
    "    !pip install accelerate -U\n",
    "\n",
    "    # Import the drive library to save your model, tokenizer and trainer to Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMFENJEVYdE3"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vDwNetWfYdE3"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from datasets import load_metric\n",
    "from datasets import Dataset\n",
    "from utils import read_data_as_sentence,map_labels_in_dataframe,tokenize_and_align_labels,get_label_mapping,get_labels_from_map,load_srl_model,load_dataset,compute_metrics,write_predictions_to_csv,compute_evaluation_metrics_from_csv, print_sentences\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from main_bert_srl import main, define_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA7OmWe_YdE4"
   },
   "source": [
    "## Step 1: Preprocess data\n",
    "\n",
    "Unlike traditional token labeling methods, which assign labels to individual words in isolation, BERT performs sequence labeling. This means BERT assigns labels to individual tokens, while taking the full sentence context in consideration.\n",
    "\n",
    "The English Universal PropBank 1.0 dataset is structured in [CoNNL-U Plus format](https://universaldependencies.org/ext-format.html), in which lines represent individual tokens. So before you can train the model, you need to extract sentences and labels from the datasets, and preprocess the sentences by removing non-argument labels.\n",
    "\n",
    "To preprocess the datasets and save the resulting DataFrame to a file, call the `read_data_as_sentence()` function, including:\n",
    "\n",
    "| Parameter name     | Required | Parameter description |\n",
    "|--------------------|:--------------:|-------------|\n",
    "| *positional 1*  (string)                 | ✅️ | The filepath for the CoNNLU dataset. |\n",
    "| *positional 2*  (string)               | ✅ | The filepath to write the preprocessed DataFrame to. |\n",
    "| `mode` (enum)                          | Optional (defaults to **basic**) | The method of preprocessing, used for the advanced model. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vyzGaxOIYdE5"
   },
   "outputs": [],
   "source": [
    "train_data = read_data_as_sentence('data/en_ewt-up-train.conllu', 'data/en_ewt-up-train.preprocessed.csv')\n",
    "dev_data = read_data_as_sentence('data/en_ewt-up-dev.conllu', 'data/en_ewt-up-dev.preprocessed.csv')\n",
    "test_data = read_data_as_sentence('data/en_ewt-up-test.conllu', 'data/en_ewt-up-test.preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61uzoDU4YdE6"
   },
   "source": [
    "The `read_data_as_sentence()` function returns DataFrames, where each row represents a sentence from the dataset passed to the function. Each sentence has been expanded based on its predicates, resulting in multiple copies of the same sentence, each focused on a different predicate.\n",
    "\n",
    "The DataFrame has two columns:\n",
    "\n",
    "- `input_form`: a list of strings, where each string represents a words in the sentence, followed by two special tokens:\n",
    "    1. A special token (`[SEP]`), which denotes the separation between the words of the sentence and the predicate form.\n",
    "    2. The predicate form, which corresponds to the `argument` values for the same row in the DataFrame.\n",
    "- `argument`: a list of strings, representing the arguments associated with each word in the sentence. The length of each list is equal to the number of words in the sentence, plus two additional elements, for the special token and predicate form. The arguments match the predicate appended to the `input_form` for the same row in the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWnvMr9OYdE6"
   },
   "source": [
    "### Explore the DataFrame\n",
    "\n",
    "Before you continue to tokenize the sentences and fine-tune the BERT model, it's time to get more familiar with our data.\n",
    "\n",
    "To explore the DataFrame, start by printing the head of the preprocessed DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OGHo8o63apSp",
    "outputId": "bb7ab2d4-e22d-4af9-c43a-4769a377b6ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3971 entries, 0 to 3970\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   input_form  3971 non-null   object\n",
      " 1   argument    3971 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 62.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(test_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtxwkQqYYdE7"
   },
   "source": [
    "The **Non-Null** count for both columns should match, indicating there are as many lists of `input_form` values as there are lists of `argument` values, namely one for each sentence.\n",
    "\n",
    "Next, print the words and their argument labels for the first 20 sentences of the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onp3jN-7YdE7",
    "outputId": "e8f659f8-ebe3-4719-e72d-8e08d33706bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "form: What            argument: _\n",
      "form: if              argument: _\n",
      "form: Google          argument: ARG1\n",
      "form: Morphed         argument: _\n",
      "form: Into            argument: _\n",
      "form: GoogleOS        argument: ARG2\n",
      "form: ?               argument: _\n",
      "----------------------------------------\n",
      "form: [SEP]           argument: None\n",
      "form: Morphed         argument: None\n",
      "\n",
      "========================================\n",
      "\n",
      "form: What            argument: _\n",
      "form: if              argument: _\n",
      "form: Google          argument: ARG0\n",
      "form: expanded        argument: _\n",
      "form: on              argument: _\n",
      "form: its             argument: _\n",
      "form: search          argument: _\n",
      "form: -               argument: _\n",
      "form: engine          argument: _\n",
      "form: (               argument: _\n",
      "form: and             argument: _\n",
      "form: now             argument: _\n",
      "form: e-mail          argument: _\n",
      "form: )               argument: _\n",
      "form: wares           argument: ARG1\n",
      "form: into            argument: _\n",
      "form: a               argument: _\n",
      "form: full            argument: _\n",
      "form: -               argument: _\n",
      "form: fledged         argument: _\n",
      "form: operating       argument: _\n",
      "form: system          argument: ARG4\n",
      "form: ?               argument: _\n",
      "----------------------------------------\n",
      "form: [SEP]           argument: None\n",
      "form: expanded        argument: None\n",
      "\n",
      "========================================\n",
      "\n",
      "form: (               argument: _\n",
      "form: And             argument: _\n",
      "form: ,               argument: _\n",
      "form: by              argument: _\n",
      "form: the             argument: _\n",
      "form: way             argument: ARGM-DIS\n",
      "form: ,               argument: _\n",
      "form: is              argument: _\n",
      "form: anybody         argument: ARG1\n",
      "form: else            argument: _\n",
      "form: just            argument: _\n",
      "form: a               argument: _\n",
      "form: little          argument: _\n",
      "form: nostalgic       argument: ARG2\n",
      "form: for             argument: _\n",
      "form: the             argument: _\n",
      "form: days            argument: _\n",
      "form: when            argument: _\n",
      "form: that            argument: _\n",
      "form: was             argument: _\n",
      "form: a               argument: _\n",
      "form: good            argument: _\n",
      "form: thing           argument: _\n",
      "form: ?               argument: _\n",
      "form: )               argument: _\n",
      "----------------------------------------\n",
      "form: [SEP]           argument: None\n",
      "form: is              argument: None\n",
      "\n",
      "========================================\n",
      "\n",
      "form: (               argument: _\n",
      "form: And             argument: _\n",
      "form: ,               argument: _\n",
      "form: by              argument: _\n",
      "form: the             argument: _\n",
      "form: way             argument: _\n",
      "form: ,               argument: _\n",
      "form: is              argument: _\n",
      "form: anybody         argument: _\n",
      "form: else            argument: _\n",
      "form: just            argument: _\n",
      "form: a               argument: _\n",
      "form: little          argument: _\n",
      "form: nostalgic       argument: _\n",
      "form: for             argument: _\n",
      "form: the             argument: _\n",
      "form: days            argument: ARGM-TMP\n",
      "form: when            argument: R-ARGM-TMP\n",
      "form: that            argument: ARG1\n",
      "form: was             argument: _\n",
      "form: a               argument: _\n",
      "form: good            argument: _\n",
      "form: thing           argument: ARG2\n",
      "form: ?               argument: _\n",
      "form: )               argument: _\n",
      "----------------------------------------\n",
      "form: [SEP]           argument: None\n",
      "form: is              argument: None\n",
      "\n",
      "========================================\n",
      "\n",
      "form: This            argument: _\n",
      "form: BuzzMachine     argument: ARG2\n",
      "form: post            argument: _\n",
      "form: argues          argument: _\n",
      "form: that            argument: _\n",
      "form: Google          argument: _\n",
      "form: 's              argument: _\n",
      "form: rush            argument: _\n",
      "form: toward          argument: _\n",
      "form: ubiquity        argument: _\n",
      "form: might           argument: _\n",
      "form: backfire        argument: _\n",
      "form: --              argument: _\n",
      "form: which           argument: _\n",
      "form: we              argument: _\n",
      "form: 've             argument: _\n",
      "form: all             argument: _\n",
      "form: heard           argument: _\n",
      "form: before          argument: _\n",
      "form: ,               argument: _\n",
      "form: but             argument: _\n",
      "form: it              argument: _\n",
      "form: 's              argument: _\n",
      "form: particularly    argument: _\n",
      "form: well            argument: _\n",
      "form: -               argument: _\n",
      "form: put             argument: _\n",
      "form: in              argument: _\n",
      "form: this            argument: _\n",
      "form: post            argument: _\n",
      "form: .               argument: _\n",
      "----------------------------------------\n",
      "form: [SEP]           argument: None\n",
      "form: post            argument: None\n",
      "\n",
      "========================================\n",
      "\n",
      "form: This            argument: _\n",
      "form: BuzzMachine     argument: _\n",
      "form: post            argument: ARG0\n",
      "form: argues          argument: _\n",
      "form: that            argument: _\n",
      "form: Google          argument: _\n",
      "form: 's              argument: _\n",
      "form: rush            argument: _\n",
      "form: toward          argument: _\n",
      "form: ubiquity        argument: _\n",
      "form: might           argument: _\n",
      "form: backfire        argument: ARG1\n",
      "form: --              argument: _\n",
      "form: which           argument: _\n",
      "form: we              argument: _\n",
      "form: 've             argument: _\n",
      "form: all             argument: _\n",
      "form: heard           argument: _\n",
      "form: before          argument: _\n",
      "form: ,               argument: _\n",
      "form: but             argument: _\n",
      "form: it              argument: _\n",
      "form: 's              argument: _\n",
      "form: particularly    argument: _\n",
      "form: well            argument: _\n",
      "form: -               argument: _\n",
      "form: put             argument: _\n",
      "form: in              argument: _\n",
      "form: this            argument: _\n",
      "form: post            argument: _\n",
      "form: .               argument: _\n",
      "----------------------------------------\n",
      "form: [SEP]           argument: None\n",
      "form: argues          argument: None\n",
      "\n",
      "========================================\n",
      "\n",
      "form: This            argument: _\n",
      "form: BuzzMachine     argument: _\n",
      "form: post            argument: _\n",
      "form: argues          argument: _\n",
      "form: that            argument: _\n",
      "form: Google          argument: ARG1\n",
      "form: 's              argument: _\n",
      "form: rush            argument: _\n",
      "form: toward          argument: _\n",
      "form: ubiquity        argument: ARG2\n",
      "form: might           argument: _\n",
      "form: backfire        argument: _\n",
      "form: --              argument: _\n",
      "form: which           argument: _\n",
      "form: we              argument: _\n",
      "form: 've             argument: _\n",
      "form: all             argument: _\n",
      "form: heard           argument: _\n",
      "form: before          argument: _\n",
      "form: ,               argument: _\n",
      "form: but             argument: _\n",
      "form: it              argument: _\n",
      "form: 's              argument: _\n",
      "form: particularly    argument: _\n",
      "form: well            argument: _\n",
      "form: -               argument: _\n",
      "form: put             argument: _\n",
      "form: in              argument: _\n",
      "form: this            argument: _\n",
      "form: post            argument: _\n",
      "form: .               argument: _\n",
      "----------------------------------------\n",
      "form: [SEP]           argument: None\n",
      "form: rush            argument: None\n",
      "\n",
      "========================================\n",
      "\n",
      "form: This            argument: _\n",
      "form: BuzzMachine     argument: _\n",
      "form: post            argument: _\n",
      "form: argues          argument: _\n",
      "form: that            argument: _\n",
      "form: Google          argument: _\n",
      "form: 's              argument: _\n",
      "form: rush            argument: ARG1\n",
      "form: toward          argument: _\n",
      "form: ubiquity        argument: _\n",
      "form: might           argument: ARGM-MOD\n",
      "form: backfire        argument: _\n",
      "form: --              argument: _\n",
      "form: which           argument: _\n",
      "form: we              argument: _\n",
      "form: 've             argument: _\n",
      "form: all             argument: _\n",
      "form: heard           argument: ARGM-ADV\n",
      "form: before          argument: _\n",
      "form: ,               argument: _\n",
      "form: but             argument: _\n",
      "form: it              argument: _\n",
      "form: 's              argument: _\n",
      "form: particularly    argument: _\n",
      "form: well            argument: _\n",
      "form: -               argument: _\n",
      "form: put             argument: _\n",
      "form: in              argument: _\n",
      "form: this            argument: _\n",
      "form: post            argument: _\n",
      "form: .               argument: _\n",
      "----------------------------------------\n",
      "form: [SEP]           argument: None\n",
      "form: backfire        argument: None\n",
      "\n",
      "========================================\n",
      "\n",
      "form: This            argument: _\n",
      "form: BuzzMachine     argument: _\n",
      "form: post            argument: _\n",
      "form: argues          argument: _\n",
      "form: that            argument: _\n",
      "form: Google          argument: _\n",
      "form: 's              argument: _\n",
      "form: rush            argument: _\n",
      "form: toward          argument: _\n",
      "form: ubiquity        argument: _\n",
      "form: might           argument: _\n",
      "form: backfire        argument: _\n",
      "form: --              argument: _\n",
      "form: which           argument: ARG1\n",
      "form: we              argument: ARG0\n",
      "form: 've             argument: _\n",
      "form: all             argument: ARGM-ADV\n",
      "form: heard           argument: _\n",
      "form: before          argument: ARGM-TMP\n",
      "form: ,               argument: _\n",
      "form: but             argument: _\n",
      "form: it              argument: _\n",
      "form: 's              argument: _\n",
      "form: particularly    argument: _\n",
      "form: well            argument: _\n",
      "form: -               argument: _\n",
      "form: put             argument: _\n",
      "form: in              argument: _\n",
      "form: this            argument: _\n",
      "form: post            argument: _\n",
      "form: .               argument: _\n",
      "----------------------------------------\n",
      "form: [SEP]           argument: None\n",
      "form: heard           argument: None\n",
      "\n",
      "========================================\n",
      "\n",
      "form: This            argument: _\n",
      "form: BuzzMachine     argument: _\n",
      "form: post            argument: _\n",
      "form: argues          argument: _\n",
      "form: that            argument: _\n",
      "form: Google          argument: _\n",
      "form: 's              argument: _\n",
      "form: rush            argument: _\n",
      "form: toward          argument: _\n",
      "form: ubiquity        argument: _\n",
      "form: might           argument: _\n",
      "form: backfire        argument: _\n",
      "form: --              argument: _\n",
      "form: which           argument: _\n",
      "form: we              argument: _\n",
      "form: 've             argument: _\n",
      "form: all             argument: _\n",
      "form: heard           argument: _\n",
      "form: before          argument: _\n",
      "form: ,               argument: _\n",
      "form: but             argument: _\n",
      "form: it              argument: ARG1\n",
      "form: 's              argument: _\n",
      "form: particularly    argument: ARGM-ADV\n",
      "form: well            argument: _\n",
      "form: -               argument: _\n",
      "form: put             argument: ARG2\n",
      "form: in              argument: _\n",
      "form: this            argument: _\n",
      "form: post            argument: ARGM-LOC\n",
      "form: .               argument: _\n",
      "----------------------------------------\n",
      "form: [SEP]           argument: None\n",
      "form: 's              argument: None\n",
      "\n",
      "========================================\n",
      "\n",
      "form: Google          argument: ARG1\n",
      "form: is              argument: _\n",
      "form: a               argument: _\n",
      "form: nice            argument: _\n",
      "form: search          argument: _\n",
      "form: engine          argument: ARG2\n",
      "form: .               argument: _\n",
      "----------------------------------------\n",
      "form: [SEP]           argument: None\n",
      "form: is              argument: None\n",
      "\n",
      "========================================\n",
      "\n",
      "form: Does            argument: _\n",
      "form: anybody         argument: ARG0\n",
      "form: use             argument: _\n",
      "form: it              argument: ARG1\n",
      "form: for             argument: _\n",
      "form: anything        argument: ARG2\n",
      "form: else            argument: _\n",
      "form: ?               argument: _\n",
      "----------------------------------------\n",
      "form: [SEP]           argument: None\n",
      "form: use             argument: None\n",
      "\n",
      "========================================\n",
      "\n",
      "form: They            argument: ARG0\n",
      "form: own             argument: _\n",
      "form: blogger         argument: ARG1\n",
      "form: ,               argument: _\n",
      "form: of              argument: ARGM-ADV\n",
      "form: course          argument: _\n",
      "form: .               argument: _\n",
      "----------------------------------------\n",
      "form: [SEP]           argument: None\n",
      "form: own             argument: None\n",
      "\n",
      "========================================\n",
      "\n",
      "form: Is              argument: _\n",
      "form: that            argument: ARG1\n",
      "form: a               argument: _\n",
      "form: money           argument: _\n",
      "form: maker           argument: ARG2\n",
      "form: ?               argument: _\n",
      "----------------------------------------\n",
      "form: [SEP]           argument: None\n",
      "form: Is              argument: None\n",
      "\n",
      "========================================\n",
      "\n",
      "form: I               argument: ARG1\n",
      "form: 'm              argument: _\n",
      "form: staying         argument: _\n",
      "form: away            argument: ARG3\n",
      "form: from            argument: _\n",
      "form: the             argument: _\n",
      "form: stock           argument: _\n",
      "form: .               argument: _\n",
      "----------------------------------------\n",
      "form: [SEP]           argument: None\n",
      "form: staying         argument: None\n",
      "\n",
      "========================================\n",
      "\n",
      "form: I               argument: ARG0\n",
      "form: doubt           argument: _\n",
      "form: the             argument: _\n",
      "form: very            argument: _\n",
      "form: few             argument: _\n",
      "form: who             argument: _\n",
      "form: actually        argument: _\n",
      "form: read            argument: _\n",
      "form: my              argument: _\n",
      "form: blog            argument: _\n",
      "form: have            argument: _\n",
      "form: not             argument: _\n",
      "form: come            argument: ARG1\n",
      "form: across          argument: _\n",
      "form: this            argument: _\n",
      "form: yet             argument: _\n",
      "form: ,               argument: _\n",
      "form: but             argument: _\n",
      "form: I               argument: _\n",
      "form: figured         argument: _\n",
      "form: I               argument: _\n",
      "form: would           argument: _\n",
      "form: put             argument: _\n",
      "form: it              argument: _\n",
      "form: out             argument: _\n",
      "form: there           argument: _\n",
      "form: anyways         argument: _\n",
      "form: .               argument: _\n",
      "----------------------------------------\n",
      "form: [SEP]           argument: None\n",
      "form: doubt           argument: None\n",
      "\n",
      "========================================\n",
      "\n",
      "form: I               argument: _\n",
      "form: doubt           argument: _\n",
      "form: the             argument: _\n",
      "form: very            argument: _\n",
      "form: few             argument: ARG0\n",
      "form: who             argument: R-ARG0\n",
      "form: actually        argument: ARGM-ADV\n",
      "form: read            argument: _\n",
      "form: my              argument: _\n",
      "form: blog            argument: ARG1\n",
      "form: have            argument: _\n",
      "form: not             argument: _\n",
      "form: come            argument: _\n",
      "form: across          argument: _\n",
      "form: this            argument: _\n",
      "form: yet             argument: _\n",
      "form: ,               argument: _\n",
      "form: but             argument: _\n",
      "form: I               argument: _\n",
      "form: figured         argument: _\n",
      "form: I               argument: _\n",
      "form: would           argument: _\n",
      "form: put             argument: _\n",
      "form: it              argument: _\n",
      "form: out             argument: _\n",
      "form: there           argument: _\n",
      "form: anyways         argument: _\n",
      "form: .               argument: _\n",
      "----------------------------------------\n",
      "form: [SEP]           argument: None\n",
      "form: read            argument: None\n",
      "\n",
      "========================================\n",
      "\n",
      "form: I               argument: _\n",
      "form: doubt           argument: _\n",
      "form: the             argument: _\n",
      "form: very            argument: _\n",
      "form: few             argument: ARG0\n",
      "form: who             argument: _\n",
      "form: actually        argument: _\n",
      "form: read            argument: _\n",
      "form: my              argument: _\n",
      "form: blog            argument: _\n",
      "form: have            argument: _\n",
      "form: not             argument: ARGM-NEG\n",
      "form: come            argument: _\n",
      "form: across          argument: _\n",
      "form: this            argument: ARG1\n",
      "form: yet             argument: ARGM-TMP\n",
      "form: ,               argument: _\n",
      "form: but             argument: _\n",
      "form: I               argument: _\n",
      "form: figured         argument: _\n",
      "form: I               argument: _\n",
      "form: would           argument: _\n",
      "form: put             argument: _\n",
      "form: it              argument: _\n",
      "form: out             argument: _\n",
      "form: there           argument: _\n",
      "form: anyways         argument: _\n",
      "form: .               argument: _\n",
      "----------------------------------------\n",
      "form: [SEP]           argument: None\n",
      "form: come            argument: None\n",
      "\n",
      "========================================\n",
      "\n",
      "form: I               argument: _\n",
      "form: doubt           argument: _\n",
      "form: the             argument: _\n",
      "form: very            argument: _\n",
      "form: few             argument: _\n",
      "form: who             argument: _\n",
      "form: actually        argument: _\n",
      "form: read            argument: _\n",
      "form: my              argument: _\n",
      "form: blog            argument: _\n",
      "form: have            argument: _\n",
      "form: not             argument: _\n",
      "form: come            argument: _\n",
      "form: across          argument: _\n",
      "form: this            argument: _\n",
      "form: yet             argument: _\n",
      "form: ,               argument: _\n",
      "form: but             argument: _\n",
      "form: I               argument: ARG0\n",
      "form: figured         argument: _\n",
      "form: I               argument: _\n",
      "form: would           argument: _\n",
      "form: put             argument: ARG1\n",
      "form: it              argument: _\n",
      "form: out             argument: _\n",
      "form: there           argument: _\n",
      "form: anyways         argument: _\n",
      "form: .               argument: _\n",
      "----------------------------------------\n",
      "form: [SEP]           argument: None\n",
      "form: figured         argument: None\n",
      "\n",
      "========================================\n",
      "\n",
      "form: I               argument: _\n",
      "form: doubt           argument: _\n",
      "form: the             argument: _\n",
      "form: very            argument: _\n",
      "form: few             argument: _\n",
      "form: who             argument: _\n",
      "form: actually        argument: _\n",
      "form: read            argument: _\n",
      "form: my              argument: _\n",
      "form: blog            argument: _\n",
      "form: have            argument: _\n",
      "form: not             argument: _\n",
      "form: come            argument: _\n",
      "form: across          argument: _\n",
      "form: this            argument: _\n",
      "form: yet             argument: _\n",
      "form: ,               argument: _\n",
      "form: but             argument: _\n",
      "form: I               argument: _\n",
      "form: figured         argument: _\n",
      "form: I               argument: ARG0\n",
      "form: would           argument: ARGM-MOD\n",
      "form: put             argument: _\n",
      "form: it              argument: ARG1\n",
      "form: out             argument: _\n",
      "form: there           argument: ARG2\n",
      "form: anyways         argument: ARGM-ADV\n",
      "form: .               argument: _\n",
      "----------------------------------------\n",
      "form: [SEP]           argument: None\n",
      "form: put             argument: None\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_sentences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWiq1HplapSq"
   },
   "source": [
    "As you can see, the sequence of word forms runs parallel to the sequence of argument labels. This means that for every index of `input_form`, the same index of `argument` gives its argument label.\n",
    "\n",
    "Argument labels are:\n",
    "- **'_'** for tokens that are not an argument (in the current predicate sense of the sentence).\n",
    "- The token's respective Propbank label for tokens that are an argument, e.g. **ARG1**\n",
    "- **None** for the special separator token (`[SEP]`) and the predicate token that follows the separator.\n",
    "\n",
    "For example, in the the first sentence of the test data printed above (\"What if Google Morphed Into GoogleOS?\"), the predicate 'Morphed' evokes [the frame `morph.01`](https://propbank.github.io/v3.4.0/frames/morph.html#morph.01). The frame's arguments are:\n",
    "\n",
    "- `ARG0-PAG`: causer of transformation\n",
    "- `ARG1-PPT`: thing changing\n",
    "- `ARG2-PRD`: end state\n",
    "- `ARG3-VSP`: start state\n",
    "\n",
    "In this example, the `ARG1` label is assigned to 'Google', and the `ARG2` label is assigned to 'GoogleOS', which indicates 'Google' is the thing that is changing and 'GoogleOS' is its end state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSXRRKutYdE7"
   },
   "source": [
    "## Step 2: Initialize a tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5g8f-8dYdE8"
   },
   "source": [
    "Now that you have extracted sentences and labels from the datasets, you need to prepare the sentences for the BERT model by tokenizing them.\n",
    "\n",
    "Use HuggingFace's [`AutoTokenizer`](https://huggingface.co/docs/transformers/v4.38.2/en/model_doc/auto#transformers.AutoTokenizer) to construct a DistilBERT tokenizer, which is based on the WordPiece algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJxQoz_GYdE8",
    "outputId": "25116b59-2c07-4441-bcae-60ccac880356"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set the model ID to use\n",
    "model_id = \"distilbert-base-uncased\"\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Check the assertion that the tokenizer is an instance of transformers.PreTrainedTokenizerFast\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vi0vG2xMYdE8"
   },
   "source": [
    "To test the `tokenizer()`, tokenize the first sentence of the test data, including:\n",
    "\n",
    "- `add_special_tokens` set to **True** to add a `[CLS]` token to the start of every sentence.\n",
    "- `is_split_into_words` set to **True** because the sentence is already split into words (based on the Universal Propbank 1.0 dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Wg7_MEckUoi",
    "outputId": "80d49153-8131-4113-d7f0-7cec86b1a96b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     [CLS] 101\n",
      "      what 2054\n",
      "        if 2065\n",
      "    google 8224\n",
      "       mor 22822\n",
      "      ##ph 8458\n",
      "      ##ed 2098\n",
      "      into 2046\n",
      "    google 8224\n",
      "      ##os 2891\n",
      "         ? 1029\n",
      "     [SEP] 102\n",
      "       mor 22822\n",
      "      ##ph 8458\n",
      "      ##ed 2098\n",
      "     [SEP] 102\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the first example in the test data\n",
    "example = test_data['input_form'][0]\n",
    "tokenized_input = tokenizer(example,add_special_tokens=True, is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "\n",
    "# Print the example tokens and their corresponding IDs\n",
    "for token, id in zip(tokens, tokenized_input[\"input_ids\"]):\n",
    "    print(f\"{token:>10} {id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOFsSEewapSr"
   },
   "source": [
    "You've successfully tokenized the sample sentence, splitting words up into subword tokens and fetching their token IDs from DistilBERT's vocabulary.\n",
    "\n",
    "> Note: notice how the special tokens `[CLS]` and `[SEP]` are tokenized as **101** and **102**. These numbers are meaningful to BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lERE6j_HYdE-"
   },
   "source": [
    "## Step 3: Prepare the input for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8BO8OyoYdE-"
   },
   "source": [
    "Before training the model, map the labels in the datasets to numerical values. This ensures consistency and facilitates the training process.\n",
    "\n",
    "To get the label mapping, call `get_label_mapping()`, including:\n",
    "\n",
    "| Parameter name     | Required | Parameter description |\n",
    "|--------------------|:--------------:|-------------|\n",
    "| *positional 1* (DataFrame)          | ✅️ | The training dataset for which to extract the label mapping. |\n",
    "| *positional 2* (DataFrame)          | ✅ | The test dataset for which to extract the label mapping.|\n",
    "| *positional 3* (DataFrame)          | ✅ | The dev dataset for which to extract the label mapping. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kangOKNiYdE-"
   },
   "outputs": [],
   "source": [
    "label_map = get_label_mapping(train_data, test_data, dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dfd-JBwwapSs"
   },
   "source": [
    "The `get_label_mapping()` function returns an alphabetically-ordered dictionary mapping:\n",
    "- **_** to **0**.\n",
    "- String labels to integers, e.g. **ARG0** to **1**.\n",
    "- **None** to **None**, to preserve the labels for special tokens and predicates. (You will replace **None** with **-100** later to mask these tokens from being labeled.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Id6U73tRYdE_",
    "outputId": "81ca2027-3c86-4404-e22b-909f236144c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_': 0, 'ARG0': 1, 'ARG1': 2, 'ARG1-DSP': 3, 'ARG2': 4, 'ARG3': 5, 'ARG4': 6, 'ARG5': 7, 'ARGA': 8, 'ARGM-ADJ': 9, 'ARGM-ADV': 10, 'ARGM-CAU': 11, 'ARGM-COM': 12, 'ARGM-CXN': 13, 'ARGM-DIR': 14, 'ARGM-DIS': 15, 'ARGM-EXT': 16, 'ARGM-GOL': 17, 'ARGM-LOC': 18, 'ARGM-LVB': 19, 'ARGM-MNR': 20, 'ARGM-MOD': 21, 'ARGM-NEG': 22, 'ARGM-PRD': 23, 'ARGM-PRP': 24, 'ARGM-PRR': 25, 'ARGM-REC': 26, 'ARGM-TMP': 27, 'C-ARG0': 28, 'C-ARG1': 29, 'C-ARG1-DSP': 30, 'C-ARG2': 31, 'C-ARG3': 32, 'C-ARG4': 33, 'C-ARGM-ADV': 34, 'C-ARGM-COM': 35, 'C-ARGM-CXN': 36, 'C-ARGM-DIR': 37, 'C-ARGM-EXT': 38, 'C-ARGM-GOL': 39, 'C-ARGM-LOC': 40, 'C-ARGM-MNR': 41, 'C-ARGM-PRP': 42, 'C-ARGM-PRR': 43, 'C-ARGM-TMP': 44, 'R-ARG0': 45, 'R-ARG1': 46, 'R-ARG2': 47, 'R-ARG3': 48, 'R-ARG4': 49, 'R-ARGM-ADJ': 50, 'R-ARGM-ADV': 51, 'R-ARGM-CAU': 52, 'R-ARGM-COM': 53, 'R-ARGM-DIR': 54, 'R-ARGM-GOL': 55, 'R-ARGM-LOC': 56, 'R-ARGM-MNR': 57, 'R-ARGM-TMP': 58, None: None}\n"
     ]
    }
   ],
   "source": [
    "print(label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiyFPfnuYdFA"
   },
   "source": [
    "\n",
    "Next, apply the label mapping to the datasets, adding the column `mapped_labels` to the DataFrames. This column contains arrays of integers representing the labels, based on the label mapping.\n",
    "\n",
    "To apply the label mapping, call `map_labels_in_dataframe()`, including:\n",
    "\n",
    "| Parameter name     | Required | Parameter description |\n",
    "|--------------------|:--------------:|-------------|\n",
    "| *positional 1*                   | ✅️ | The DataFrame for which to convert the argument labels. |\n",
    "| *positional 2*                 | ✅ | The label mapping, created with `get_label_mapping()`. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "N1Cy6U0nYdFB"
   },
   "outputs": [],
   "source": [
    "train_data = map_labels_in_dataframe(train_data, label_map)\n",
    "dev_data = map_labels_in_dataframe(dev_data, label_map)\n",
    "test_data = map_labels_in_dataframe(test_data, label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1MzLjuoYdFB"
   },
   "source": [
    "As you can see, for each row in the DataFrame, the values in `mapped_labels` and `arguments` correspond to the mapping in `label_map`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "NrsuWwugYdFB",
    "outputId": "99ce8722-5d7b-48a9-ba26-78b853c77ea3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"test_data\",\n  \"rows\": 3971,\n  \"fields\": [\n    {\n      \"column\": \"input_form\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"argument\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mapped_labels\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "test_data"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-89c3d3eb-d661-418f-81ae-4c91bd397d8e\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_form</th>\n",
       "      <th>argument</th>\n",
       "      <th>mapped_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[What, if, Google, Morphed, Into, GoogleOS, ?,...</td>\n",
       "      <td>[_, _, ARG1, _, _, ARG2, _, None, None]</td>\n",
       "      <td>[0, 0, 2, 0, 0, 4, 0, None, None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[What, if, Google, expanded, on, its, search, ...</td>\n",
       "      <td>[_, _, ARG0, _, _, _, _, _, _, _, _, _, _, _, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(, And, ,, by, the, way, ,, is, anybody, else...</td>\n",
       "      <td>[_, _, _, _, _, ARGM-DIS, _, _, ARG1, _, _, _,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 15, 0, 0, 2, 0, 0, 0, 0, 4, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(, And, ,, by, the, way, ,, is, anybody, else...</td>\n",
       "      <td>[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[This, BuzzMachine, post, argues, that, Google...</td>\n",
       "      <td>[_, ARG2, _, _, _, _, _, _, _, _, _, _, _, _, ...</td>\n",
       "      <td>[0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89c3d3eb-d661-418f-81ae-4c91bd397d8e')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-89c3d3eb-d661-418f-81ae-4c91bd397d8e button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-89c3d3eb-d661-418f-81ae-4c91bd397d8e');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-b56bff38-f0f7-4048-99d3-af1f2e0fccee\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b56bff38-f0f7-4048-99d3-af1f2e0fccee')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-b56bff38-f0f7-4048-99d3-af1f2e0fccee button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                          input_form  \\\n",
       "0  [What, if, Google, Morphed, Into, GoogleOS, ?,...   \n",
       "1  [What, if, Google, expanded, on, its, search, ...   \n",
       "2  [(, And, ,, by, the, way, ,, is, anybody, else...   \n",
       "3  [(, And, ,, by, the, way, ,, is, anybody, else...   \n",
       "4  [This, BuzzMachine, post, argues, that, Google...   \n",
       "\n",
       "                                            argument  \\\n",
       "0            [_, _, ARG1, _, _, ARG2, _, None, None]   \n",
       "1  [_, _, ARG0, _, _, _, _, _, _, _, _, _, _, _, ...   \n",
       "2  [_, _, _, _, _, ARGM-DIS, _, _, ARG1, _, _, _,...   \n",
       "3  [_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, ...   \n",
       "4  [_, ARG2, _, _, _, _, _, _, _, _, _, _, _, _, ...   \n",
       "\n",
       "                                       mapped_labels  \n",
       "0                  [0, 0, 2, 0, 0, 4, 0, None, None]  \n",
       "1  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...  \n",
       "2  [0, 0, 0, 0, 0, 15, 0, 0, 2, 0, 0, 0, 0, 4, 0,...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78d8pDrEYdFB"
   },
   "source": [
    "Now that you have initialized and tested the `tokenizer()` and added mapped labels to the DataFrames, it's time to tokenize (and pad) all sentences.\n",
    "\n",
    "Since WordPiece tokenization potentially breaks words up into subword tokens, the tokens and their labels have to be re-aligned. The `tokenize_and_align_labels()` function you'll call for this iterates over each token and determines the appropriate label based on the provided dataset.\n",
    "\n",
    "Special tokens are assigned a label of **-100** to indicate they should be ignored in the loss function. Labels for the first token of each word are set accordingly, while labels for subsequent tokens within the same word are determined based on the `label_all_tokens` flag.\n",
    "\n",
    "To tokenize the sentences and align the labels, call `tokenize_and_align_labels()`, including:\n",
    "\n",
    "| Parameter name     | Required | Parameter description |\n",
    "|--------------------|:--------------:|-------------|\n",
    "| *positional 1* (`transformers AutoTokenizer`) | ✅️ | The `tokenizer()` for the pre-trained model. |\n",
    "| *positional 2* (DataFrame) | ✅ | The preprocessed datasets |\n",
    "| `label_all_tokens` (boolean)     | Optional (defaults to **True**) | Whether all tokens should receive their own label, accounting for words split into subtokens |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OHRNa98wYdFB"
   },
   "outputs": [],
   "source": [
    "tokenized_test = tokenize_and_align_labels(tokenizer, test_data, label_all_tokens=True)\n",
    "tokenized_train = tokenize_and_align_labels(tokenizer, train_data, label_all_tokens=True)\n",
    "tokenized_dev = tokenize_and_align_labels(tokenizer, dev_data, label_all_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9WeeG8YYdFB"
   },
   "source": [
    "Now that you have tokenized all three datasets, let's examine the result.\n",
    "\n",
    "The `tokenized_` datasets are of the type `transformers.tokenization_utils_base.BatchEncoding` and have three attributes per row:\n",
    "\n",
    "1. `input_ids`: an array of token IDs for the tokenized sentence. Starts with the token ID for the `[CLS]` token, followed by the tokenized sentence, the `[SEP]` token, the predicate, and a final `[SEP]` token.\n",
    "2. `attention_mask`: an array representing the attention mask for the sentence.\n",
    "3. `labels`: an array with numerical labels, aligned with the tokens.\n",
    "\n",
    "> Note: all three arrays are padded so that every sample per dataset is of equal length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e23nB-4dYdFB",
    "outputId": "798f512e-de56-4a9a-d144-4445a7899bed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "['[CLS]', 'what', 'if', 'google', 'mor', '##ph', '##ed', 'into', 'google', '##os', '?', '[SEP]', 'mor', '##ph', '##ed', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "input_ids: tensor([  101,  2054,  2065,  8224, 22822,  8458,  2098,  2046,  8224,  2891,\n",
      "         1029,   102, 22822,  8458,  2098,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0])\n",
      "attention_mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "labels: [-100, 0, 0, 2, 0, 0, 0, 0, 4, 4, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenized_test))\n",
    "print(tokenized_test.keys())\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_test[\"input_ids\"][0]))\n",
    "for key in tokenized_test.keys():\n",
    "    print(f\"{key}: {tokenized_test[key][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j31tpMcxYdFB"
   },
   "source": [
    "To confirm that you have padded all sentences in the `tokenized_test` dataset to be of equal length, let's check the length of all three arrays for the first 10 sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vp6JKWx6YdFC",
    "outputId": "1a13d704-e7cf-418d-dae9-4faa4eeaecc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 0: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
      "sentence 1: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
      "sentence 2: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
      "sentence 3: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
      "sentence 4: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
      "sentence 5: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
      "sentence 6: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
      "sentence 7: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
      "sentence 8: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
      "sentence 9: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"sentence {i}:\", \"input_ids:\", len(tokenized_test[\"input_ids\"][i]), \"\\tlabels:\", len(tokenized_test[\"labels\"][i]), \"\\tattention_mask:\", len(tokenized_test[\"attention_mask\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uui9puOfYdFC"
   },
   "source": [
    "Converting the tokenized data to datasets format with the function `load_dataset`\n",
    "\n",
    "Now that you have tokenized and padded the sentences, and aligned the labels with the tokens, you're ready to transform the tokenized datasets into Hugging Face's [`datasets.arrow_dataset.Dataset`](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset).\n",
    "\n",
    "To transform the tokenized datasets into `Dataset` objects, call the `load_dataset()` function, which calls the [`Dataset.from_dict()` method](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.from_dict), including:\n",
    "\n",
    "| Parameter name     | Required | Parameter description |\n",
    "|--------------------|:--------------:|-------------|\n",
    "| *positional 1* (`transformers.tokenization_utils_base.BatchEncoding`) | ✅️ | The tokenized dataset. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "kBKxTmz3YdFC"
   },
   "outputs": [],
   "source": [
    "dataset_train = load_dataset(tokenized_train)\n",
    "dataset_dev = load_dataset(tokenized_dev)\n",
    "dataset_test = load_dataset(tokenized_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZQRfvuPapSv"
   },
   "source": [
    "Let's print the type of the resulting dataset, to confirm the transformation into `datasets.arrow_dataset.Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VX4HNMdEapSv",
    "outputId": "ec4ace68-d432-4b01-a64d-5164390623b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-Pl5GvcYdFC"
   },
   "source": [
    "## Step 4: Fine-tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5yTu8JzYdFC"
   },
   "source": [
    "Finally, the sentences have been transformed from CoNNL-U Plus format to Hugging Face `Dataset` objects: it's time to fine-tune BERT!\n",
    "\n",
    "Fine-tuning a BERT model on the full dataset can be a very computationally challenging task. To speed up the process, create subsets of the three datasets with 1000 samples per dataset, selected randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "SLNvuvOvYdFD"
   },
   "outputs": [],
   "source": [
    "#small_train_dataset = dataset_train.shuffle(seed=42).select(range(1000))\n",
    "#small_eval_dataset = dataset_dev.shuffle(seed=42).select(range(1000))\n",
    "#small_test_dataset = dataset_test.shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BzGFWMKYdFE"
   },
   "source": [
    "To map the numerical labels back to their string representations, you need to convert the `label_map` dictionary to a list of labels (as strings).\n",
    "\n",
    "To convert the `label_map` to a list of labels (as strings), call the `get_labels_from_map()` function, including:\n",
    "\n",
    "| Parameter name     | Required | Parameter description |\n",
    "|--------------------|:--------------:|-------------|\n",
    "| *positional 1* (dictionary) | ✅️ | The dictionary mapping labels as strings to their numerical represenation. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "qGc0sBr1YdFE"
   },
   "outputs": [],
   "source": [
    "label_list = get_labels_from_map(label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCsgWLXQYdFE"
   },
   "source": [
    "Next, load the pretrained DistilBERT model using the [`AutoModelForTokenClassification.from_pretrained()` method](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained) from the `transformers` library, together with the model name (**distilbert-base-uncased**), and the [`TrainingArguments`](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/trainer#transformers.TrainingArguments) neccesary for training.\n",
    "\n",
    "To get the model, model name and `TrainingArguments`, call the `load_srl_model()` function, including:\n",
    "\n",
    "| Parameter name     | Required | Parameter description |\n",
    "|--------------------|:--------------:|-------------|\n",
    "| *positional 1* (string) | ✅️ | The model identifier.  |\n",
    "| *positional 2* (list of strings) | ✅️ | The tokenized dataset. |\n",
    "| `batch_size` (integer) | Optional (defaults to **16**) | The [batch size for training](https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#methods-and-tools-for-efficient-training-on-a-single-gpu) and [inference](https://huggingface.co/docs/setfit/main/en/how_to/batch_sizes). |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t8OACqyhYdFF",
    "outputId": "839baca7-72bb-49ab-81f5-6a80b8b46719"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model, args = load_srl_model(model_id, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFTYKnm1YdFF"
   },
   "source": [
    "Now that you have a DistilBERT model, it's time for fine-tuning the model for the task of semantic role labeling (SRL).\n",
    "\n",
    "To fine-tune your model, instantiate a [`Trainer` object](https://huggingface.co/docs/transformers/main_classes/trainer#api-reference%20][%20transformers.Trainer) from the `transformers` library, passing the `model`, `args`, `tokenizer` and datasets for training and inference. Then, call the [`Trainer.train()` method](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.train) to start the fine-tuning process.\n",
    "\n",
    "> Note: this process may take up to several hours, depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "tZVjfPfuYdFF",
    "outputId": "bcd45043-85ba-4910-9b18-2770a1bbb6a5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2096' max='2096' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2096/2096 09:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.170100</td>\n",
       "      <td>0.181336</td>\n",
       "      <td>0.302991</td>\n",
       "      <td>0.257266</td>\n",
       "      <td>0.265285</td>\n",
       "      <td>0.951306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory distilbert-base-uncased-finetuned-srl/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory distilbert-base-uncased-finetuned-srl/checkpoint-1000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory distilbert-base-uncased-finetuned-srl/checkpoint-1500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory distilbert-base-uncased-finetuned-srl/checkpoint-2000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2096, training_loss=0.24107087704971547, metrics={'train_runtime': 576.8723, 'train_samples_per_second': 58.108, 'train_steps_per_second': 3.633, 'total_flos': 1549860696773034.0, 'train_loss': 0.24107087704971547, 'epoch': 1.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=dataset_train,\n",
    "        eval_dataset=dataset_dev,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda p: compute_metrics(*p, label_list)\n",
    "    )\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXZ8ro_iYdFF"
   },
   "source": [
    "Now that you have fine-tuned the model, let's evaluate its performance on the `eval_dataset` that you set when constructing the `Trainer` instance.\n",
    "\n",
    "To evaluate the fine-tuned model, call the [`Trainer.evaluate()` method](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.evaluate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "XUpcBt68YdFG",
    "outputId": "5c0a3cf9-ebe2-424e-d060-e895b9a3f72b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='259' max='259' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [259/259 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18133634328842163, 'eval_precision': 0.3029914154827191, 'eval_recall': 0.2572655540647563, 'eval_f1': 0.26528470539015453, 'eval_accuracy': 0.9513056407397082, 'eval_runtime': 14.0247, 'eval_samples_per_second': 295.408, 'eval_steps_per_second': 18.467, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9NFI6DmYdFG"
   },
   "source": [
    "Now that you have fine-tuned your DistilBERT model for semantic role labeling, and evaluated its performance on the development dataset, it's time to infer the argument labels of the test dataset and compute a summary of the performance metrics.\n",
    "\n",
    "First, call the [`Trainer.predict()` method](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.predict) passing the test dataset. The method returns a tuple consisting of the model's predictions on the test dataset, the labels, and metrics.\n",
    "\n",
    "To compute a summary of the model's perfomance metrics on the test dataset, call the `compute_metrics()` function, including:\n",
    "\n",
    "| Parameter name     | Required | Parameter description |\n",
    "|--------------------|:--------------:|-------------|\n",
    "| *positional 1* (`np.ndarray`) | ✅️ | The array of predictions as returned from the `Trainer.predict()` method. |\n",
    "| *positional 2* (`np.ndarray`) | ✅️ | The array of argument labels as returned from the `Trainer.predict()` method. |\n",
    "| *positional 3* (list of strings) | ✅️ | The list of argument labels as strings. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "tMNBhvOcYdFG",
    "outputId": "a03b474b-d546-4d45-f4dd-81992dcdcb58"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.32850815826663876, 'recall': 0.27096924894336905, 'f1': 0.2790458165617792, 'accuracy': 0.9518960938423989}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(dataset_test)\n",
    "results = compute_metrics(predictions, labels, label_list)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrvtvLY9YdFG"
   },
   "source": [
    "Writing the predictions together with the gold labels to a csv file with the function `write_predictions_to_csv` so that the metrics per class can be computed with the `compute_evaluation_metrics_from_csv` function.\n",
    "\n",
    "Now that you have succesfully fine-tuned a model and inferred the argument labels of the test set, let's store the results on disc in a CSV file, and create a full classification report.\n",
    "\n",
    "To write the predictions to CSV, call the `write_predictions_to_csv()` function, including:\n",
    "\n",
    "| Parameter name     | Required | Parameter description |\n",
    "|--------------------|:--------------:|-------------|\n",
    "| *positional 1* (`np.ndarray`) | ✅️ | The array of predictions as returned from the `Trainer.predict()` method. |\n",
    "| *positional 2* (`np.ndarray`) | ✅️ | The array of argument labels as returned from the `Trainer.predict()` method. |\n",
    "| *positional 3* (list of strings) | ✅️ | The list of argument labels as strings. |\n",
    "| *positional 3* (string) | ✅️ | The filepath to write the results to (in CSV format). |\n",
    "\n",
    "Next, to compute a full classification report of the model's performance on the test dataset, call the `compute_evaluation_metrics_from_csv()` function, including:\n",
    "\n",
    "| Parameter name     | Required | Parameter description |\n",
    "|--------------------|:--------------:|-------------|\n",
    "| *positional 1* (string) | ✅️ | The filepath for the CSV file where the model's predictions are stored. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L8J6utkmYdFG",
    "outputId": "39c1b369-1359-41da-c7ac-f49bba6fc138"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ARG0       0.78      0.82      0.80      1899\n",
      "        ARG1       0.76      0.82      0.79      3647\n",
      "    ARG1-DSP       0.00      0.00      0.00         4\n",
      "        ARG2       0.62      0.61      0.62      1285\n",
      "        ARG3       0.00      0.00      0.00        79\n",
      "        ARG4       1.00      0.09      0.16        68\n",
      "        ARG5       0.00      0.00      0.00         1\n",
      "        ARGA       0.00      0.00      0.00         2\n",
      "    ARGM-ADJ       0.66      0.69      0.67       254\n",
      "    ARGM-ADV       0.67      0.39      0.49       526\n",
      "    ARGM-CAU       0.00      0.00      0.00        48\n",
      "    ARGM-COM       0.00      0.00      0.00        16\n",
      "    ARGM-CXN       0.00      0.00      0.00        12\n",
      "    ARGM-DIR       0.33      0.11      0.16        47\n",
      "    ARGM-DIS       0.69      0.59      0.64       196\n",
      "    ARGM-EXT       0.78      0.70      0.74       105\n",
      "    ARGM-GOL       0.00      0.00      0.00        29\n",
      "    ARGM-LOC       0.52      0.46      0.49       259\n",
      "    ARGM-LVB       0.69      0.71      0.70        69\n",
      "    ARGM-MNR       0.49      0.28      0.36       164\n",
      "    ARGM-MOD       0.86      0.97      0.91       468\n",
      "    ARGM-NEG       0.86      0.94      0.90       392\n",
      "    ARGM-PRD       0.00      0.00      0.00        50\n",
      "    ARGM-PRP       0.52      0.20      0.28        82\n",
      "    ARGM-PRR       0.67      0.03      0.05        77\n",
      "    ARGM-TMP       0.70      0.70      0.70       586\n",
      "      C-ARG0       0.00      0.00      0.00         3\n",
      "      C-ARG1       0.00      0.00      0.00        57\n",
      "  C-ARG1-DSP       0.00      0.00      0.00         1\n",
      "      C-ARG2       0.00      0.00      0.00         7\n",
      "      C-ARG3       0.00      0.00      0.00         2\n",
      "  C-ARGM-CXN       0.00      0.00      0.00         5\n",
      "  C-ARGM-LOC       0.00      0.00      0.00         1\n",
      "      R-ARG0       0.86      0.82      0.84        67\n",
      "      R-ARG1       0.68      0.75      0.72        52\n",
      "      R-ARG2       0.00      0.00      0.00         1\n",
      "  R-ARGM-ADJ       0.00      0.00      0.00         1\n",
      "  R-ARGM-ADV       0.00      0.00      0.00         1\n",
      "  R-ARGM-DIR       0.00      0.00      0.00         1\n",
      "  R-ARGM-LOC       0.00      0.00      0.00         9\n",
      "  R-ARGM-MNR       0.00      0.00      0.00         8\n",
      "  R-ARGM-TMP       0.00      0.00      0.00         2\n",
      "           _       0.98      0.98      0.98     82424\n",
      "\n",
      "    accuracy                           0.95     93007\n",
      "   macro avg       0.33      0.27      0.28     93007\n",
      "weighted avg       0.95      0.95      0.95     93007\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "results_file = \"predictions.csv\"\n",
    "write_predictions_to_csv(predictions, labels, label_list, results_file)\n",
    "classification_report = compute_evaluation_metrics_from_csv(\"predictions.csv\")\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rBk6GBWYdFG"
   },
   "source": [
    "Finally, store the `tokenizer`, `trainer` and `model` on disc using their built-in methods. For each method call, pass a string representing the directory to save the object and its configuration to.\n",
    "\n",
    "This let's you use the objects' built-in `from_pretrained()` methods to reload their state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Cni2yCZiYdFG"
   },
   "outputs": [],
   "source": [
    "# Use these codes to save model:\n",
    "tokenizer.save_pretrained(\"tokenizer.save_pretrained.distillbert-base-uncased-finetuned-srl\")\n",
    "trainer.save_model(\"trainer.save_model.distillbert-base-uncased-finetuned-srl\")\n",
    "model.save_pretrained(\"model.save_pretrained.distillbert-base-uncased-finetuned-srl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVl8CGcAapSx"
   },
   "source": [
    "If you are running this notebook in Google colab, create a directory in your Google Drive and copy the `tokenizer`, `trainer`, and `model` to your Google Drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "avWiTIQcjubr"
   },
   "outputs": [],
   "source": [
    "if runs_in_colab:\n",
    "    !mkdir -p \"/content/drive/MyDrive/NLP_3_baseline_model/model\"\n",
    "    !cp -r '/content/trainer.save_model.distillbert-base-uncased-finetuned-srl' '/content/drive/MyDrive/NLP_3_baseline_model/model'\n",
    "    !cp -r '/content/model.save_pretrained.distillbert-base-uncased-finetuned-srl' '/content/drive/MyDrive/NLP_3_baseline_model/model'\n",
    "    !cp -r '/content/tokenizer.save_pretrained.distillbert-base-uncased-finetuned-srl' '/content/drive/MyDrive/NLP_3_baseline_model/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jUYZ6_djl3mh",
    "outputId": "c4db77f5-9e15-4dd5-acb3-7cfe368d2029"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ARG0       0.79      0.82      0.80      1899\n",
      "        ARG1       0.75      0.83      0.79      3647\n",
      "    ARG1-DSP       0.00      0.00      0.00         4\n",
      "        ARG2       0.60      0.65      0.62      1285\n",
      "        ARG3       0.00      0.00      0.00        79\n",
      "        ARG4       0.00      0.00      0.00        68\n",
      "        ARG5       0.00      0.00      0.00         1\n",
      "        ARGA       0.00      0.00      0.00         2\n",
      "    ARGM-ADJ       0.68      0.68      0.68       254\n",
      "    ARGM-ADV       0.62      0.41      0.49       526\n",
      "    ARGM-CAU       1.00      0.02      0.04        48\n",
      "    ARGM-COM       0.00      0.00      0.00        16\n",
      "    ARGM-CXN       0.00      0.00      0.00        12\n",
      "    ARGM-DIR       0.27      0.15      0.19        47\n",
      "    ARGM-DIS       0.70      0.56      0.62       196\n",
      "    ARGM-EXT       0.74      0.69      0.71       105\n",
      "    ARGM-GOL       0.00      0.00      0.00        29\n",
      "    ARGM-LOC       0.54      0.48      0.51       259\n",
      "    ARGM-LVB       0.67      0.51      0.58        69\n",
      "    ARGM-MNR       0.47      0.34      0.40       164\n",
      "    ARGM-MOD       0.87      0.95      0.91       468\n",
      "    ARGM-NEG       0.85      0.94      0.90       392\n",
      "    ARGM-PRD       0.00      0.00      0.00        50\n",
      "    ARGM-PRP       0.61      0.23      0.34        82\n",
      "    ARGM-PRR       0.00      0.00      0.00        77\n",
      "    ARGM-TMP       0.71      0.74      0.73       586\n",
      "      C-ARG0       0.00      0.00      0.00         3\n",
      "      C-ARG1       0.00      0.00      0.00        57\n",
      "  C-ARG1-DSP       0.00      0.00      0.00         1\n",
      "      C-ARG2       0.00      0.00      0.00         7\n",
      "      C-ARG3       0.00      0.00      0.00         2\n",
      "  C-ARGM-CXN       0.00      0.00      0.00         5\n",
      "  C-ARGM-LOC       0.00      0.00      0.00         1\n",
      "      R-ARG0       0.83      0.85      0.84        67\n",
      "      R-ARG1       0.60      0.77      0.67        52\n",
      "      R-ARG2       0.00      0.00      0.00         1\n",
      "  R-ARGM-ADJ       0.00      0.00      0.00         1\n",
      "  R-ARGM-ADV       0.00      0.00      0.00         1\n",
      "  R-ARGM-DIR       0.00      0.00      0.00         1\n",
      "  R-ARGM-LOC       0.00      0.00      0.00         9\n",
      "  R-ARGM-MNR       0.00      0.00      0.00         8\n",
      "  R-ARGM-TMP       0.00      0.00      0.00         2\n",
      "           _       0.98      0.98      0.98     82424\n",
      "\n",
      "    accuracy                           0.95     93007\n",
      "   macro avg       0.31      0.27      0.27     93007\n",
      "weighted avg       0.95      0.95      0.95     93007\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "classification_report = compute_evaluation_metrics_from_csv(\"predictions-adv.csv\")\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of the basic model\n",
    "\n",
    "In the basic model, input sequences start with the token ID for the `[CLS]` token, followed by the tokenized sentence, the `[SEP]` token, the predicate, and a final `[SEP]` token. However, this method introduces ambiguity when there are multiple word forms in a sentence that are identical to the predicate word form, for example:\n",
    "\n",
    "```\n",
    "['[CLS]', 'she', 'saw', 'a', 'man', 'saw', 'a', 'tree', '[SEP]', 'saw', '[SEP]']\n",
    "```\n",
    "\n",
    "In the sentence above, the predicate word form 'saw' occurs twice in the sentence, with different frames and arguments for each occurence:\n",
    "1. In the first occurence, the frame evoked is that of [`see.01` *(view)*](https://propbank.github.io/v3.4.0/frames/see.html#see.01), which has the following arguments:\n",
    "\n",
    "    - `ARG0-PAG`: viewer\n",
    "    - `ARG1-PPT`: thing viewed\n",
    "    - `ARG2-PRD`: attribute of arg1, further description\n",
    "\n",
    "  - In this frame, 'she' is labeled `ARG0` and 'man' is labeled `ARG1`.\n",
    "\n",
    "2. In the second occurence, the frame evoked is that of [`saw.06` *(cut with a saw)*](https://propbank.github.io/v3.4.0/frames/see.html#saw.06), which has the following arguments:\n",
    "\n",
    "    - `ARG0-PAG`: cutter\n",
    "    - `ARG1-PPT`: thing cut\n",
    "    - `ARG2-SRC`: Cut from what? source \n",
    "\n",
    "  - In this frame, 'man' is labeled `ARG0` and 'tree' is labeled `ARG1`.\n",
    "\n",
    "Due to limitations in how your basic model represents these input sequences, the model isn't able to disambiguate between the two frames in the sentence above, introducing noise in the model. \n",
    "\n",
    "## Advanced model\n",
    "\n",
    "To resolve the limitation of the basic model, you're going to create an advanced model, using a more sophisticated representation of the input sequnece. Instead of providing just the predicate between `[SEP]` tokens, you're going to provide a context window for the predicate. \n",
    "\n",
    "This context helps the model disambiguate between the different word forms identical to the predicate word forms. The size of the context window is 3 words, meaning that alongside the predicate, its preceding and following word and put between the `[SEP]` tokens. For example, for the first occurence of 'saw', you'll create the following input sequence:\n",
    "\n",
    "```\n",
    "['[CLS]', 'she', 'saw', 'a', 'man', 'saw', 'a', 'tree', '[SEP]', 'she', 'saw', 'a' '[SEP]']\n",
    "```\n",
    "\n",
    "For the second occurence of 'saw', you'll create the following input sequence:\n",
    "\n",
    "```\n",
    "['[CLS]', 'she', 'saw', 'a', 'man', 'saw', 'a', 'tree', '[SEP]', 'man', 'saw', 'a' '[SEP]']\n",
    "```\n",
    "\n",
    "Using this context window, your model can learn to disambiguate between both occurences, reducing noise in the fine-tuning and (hopefully) improving the model's performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'define_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m args \u001b[38;5;241m=\u001b[39m define_args(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madvanced\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'define_args' is not defined"
     ]
    }
   ],
   "source": [
    "args = define_args(mode='advanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(args,mode='advanced', data_range=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations in the advanced model\n",
    "\n",
    "Although the advanced model's representation addresses the issue of predicate ambiguity, it does not provide additional information about the tokens and their probable semantic role. Such representations do not entail explicit feature-based information depicting the relations between the predicate and the tokens, thus token classification is dependent on the prertained encodings of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and future research\n",
    "\n",
    "The results show that the baseline model, taking just the predicate, performs better than the advanced model. It is likely that the context window is not meaningful enough for the model to predict the correct arguments for semantic role labeling.\n",
    "\n",
    "It is important to consider that the labels are assigned in terms of the subtokens fed to the transformer model, rather than the token itself, which might significantly impact the results. Future experiments should consider counting the label assigned to the token to evaluate the model's performance.\n",
    "\n",
    "Additionally, future research should focus on different ways to provide a meaningful representation of the predicate to the model, making it robust enough for the semantic role labeling task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGd8F8QGYdFH"
   },
   "source": [
    "## Group Contribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HHdp36FYdFH"
   },
   "source": [
    "##### Ariana Britez:\n",
    "- functions to map the labels to number for model input: get_label_mapping, map_labels_to_numbers, map_labels_in_dataframe\n",
    "- function to get the list of labels for model input: get_labels_from_map\n",
    "- function to compute the metrics during training, evaluation and inference: compute_metrics, compute_evaluation_metrics_from_csv\n",
    "- function to load the transformer model for fine-tuning: load_srl_model\n",
    "- function to load the dataset in format that model can handle: load_dataset\n",
    "- function to save the model predictions with gold labels for evaluation: write_predictions_to_csv\n",
    "- writing markdown from importing the model section until evaluation of the baseline model\n",
    "- conclusion and future research\n",
    "\n",
    "##### Kris Stallenberg:\n",
    "- Writing tutorial-style documentation for the full pipeline.\n",
    "- Adding print statements throughout the notebook clarifying the data structures.\n",
    "- Explain the limitations of the basic model and how the advanced model solves these.\n",
    "- Refactoring functions in the utils.py module.\n",
    "\n",
    "##### Farnaz Bani Fatemi:\n",
    "- Helping in preprocessing of data and fine tuning baseline model and tokenizer.\n",
    "- Function to load and represent data for baseline model input form and extract gold arguments: read_data_as_sentence.\n",
    "- Function to tokenize input form and assign each token label to its subtokens: tokenize_and_align_labels.\n",
    "- Cell in main.ipynb to locally save fine-tuned model.\n",
    "- Adding some commands to run code in colab.\n",
    "- Testing some codes to help fix debugs and issues for models.\n",
    "\n",
    "##### Szabolcs Pal\n",
    "- Helped in preprocessing the data for advanced model\n",
    "- Limitations and explanation of advanced model\n",
    "- Main function used for advanced model\n",
    "- Attemped to do postprocessing on the subtokens of the model output".\n",
    "\n",
    "##### Christina Karavida\n",
    "- Worked on the baseline model (run into issues in the evaluation stage)
    "- Also created a version of the main function for the advanced model"
    "- Explained the results for the two models"\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kd5G74joapSx"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
