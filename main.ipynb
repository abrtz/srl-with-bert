{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmZ_eppHYdEy"
      },
      "source": [
        "# Semantic role labeling with BERT\n",
        "\n",
        "In this notebook, you'll perform semantic role labeling with BERT, using the [English Universal Propbank 1.0 datasets](https://github.com/UniversalPropositions/UP-1.0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XJQ6KZ5YdE1"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Install libraries\n",
        "\n",
        "If you run this notebook in [Google colab](https://colab.research.google.com), make sure to install the required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    runs_in_colab =  True\n",
        "except ImportError:\n",
        "    runs_in_colab = False\n",
        "\n",
        "if runs_in_colab:\n",
        "    !pip install datasets\n",
        "    !pip install seqeval\n",
        "    !pip install accelerate==0.21.0\n",
        "    !pip install transformers[torch]\n",
        "    !pip install accelerate -U\n",
        "    \n",
        "    # Import the drive library to save your model, tokenizer and trainer to Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMFENJEVYdE3"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vDwNetWfYdE3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-07 22:02:53.099752: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
        "from datasets import load_metric\n",
        "from datasets import Dataset\n",
        "from utils import read_data_as_sentence,map_labels_in_dataframe,tokenize_and_align_labels,get_label_mapping,get_labels_from_map,load_srl_model,load_dataset,compute_metrics,write_predictions_to_csv,compute_evaluation_metrics_from_csv, print_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA7OmWe_YdE4"
      },
      "source": [
        "## Step 1: Preprocess data\n",
        "\n",
        "Unlike traditional token labeling methods, which assign labels to individual words in isolation, BERT performs sequence labeling. This means BERT assigns labels to individual tokens, while taking the full sentence context in consideration. \n",
        "\n",
        "The English Universal PropBank 1.0 dataset is structured in [CoNNL-U Plus format](https://universaldependencies.org/ext-format.html), in which lines represent individual tokens. So before you can train the model, you need to extract sentences and labels from the datasets, and preprocess the sentences by removing non-argument labels.\n",
        "\n",
        "To preprocess the datasets and save the resulting DataFrame to a file, call the `read_data_as_sentence()` function, including:\n",
        "\n",
        "| Parameter name     | Required | Parameter description |\n",
        "|--------------------|:--------------:|-------------|\n",
        "| *positional 1*  (string)                 | ✅️ | The filepath for the CoNNLU dataset. |\n",
        "| *positional 2*  (string)               | ✅ | The filepath to write the preprocessed DataFrame to. |\n",
        "| `mode` (enum)                          | Optional (defaults to **basic**) | The method of preprocessing, used for the advanced model. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vyzGaxOIYdE5"
      },
      "outputs": [],
      "source": [
        "train_data = read_data_as_sentence('data/en_ewt-up-train.conllu', 'data/en_ewt-up-train.preprocessed.csv')\n",
        "dev_data = read_data_as_sentence('data/en_ewt-up-dev.conllu', 'data/en_ewt-up-dev.preprocessed.csv')\n",
        "test_data = read_data_as_sentence('data/en_ewt-up-test.conllu', 'data/en_ewt-up-test.preprocessed.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61uzoDU4YdE6"
      },
      "source": [
        "The `read_data_as_sentence()` function returns DataFrames, where each row represents a sentence from the dataset passed to the function. Each sentence has been expanded based on its predicates, resulting in multiple copies of the same sentence, each focused on a different predicate.\n",
        "\n",
        "The DataFrame has two columns:\n",
        "\n",
        "- `input_form`: a list of strings, where each string represents a words in the sentence, followed by two special tokens:\n",
        "    1. A special token (`[SEP]`), which denotes the separation between the words of the sentence and the predicate form.\n",
        "    2. The predicate form, which corresponds to the `argument` values for the same row in the DataFrame.\n",
        "- `argument`: a list of strings, representing the arguments associated with each word in the sentence. The length of each list is equal to the number of words in the sentence, plus two additional elements, for the special token and predicate form. The arguments match the predicate appended to the `input_form` for the same row in the DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWnvMr9OYdE6"
      },
      "source": [
        "### Explore the DataFrame\n",
        "\n",
        "Before you continue to tokenize the sentences and fine-tune the BERT model, it's time to get more familiar with our data.\n",
        "\n",
        "To explore the DataFrame, start by printing the head of the preprocessed DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3971 entries, 0 to 3970\n",
            "Data columns (total 2 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   input_form  3971 non-null   object\n",
            " 1   argument    3971 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 62.2+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(test_data.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtxwkQqYYdE7"
      },
      "source": [
        "The **Non-Null** count for both columns should match, indicating there are as many lists of `input_form` values as there are lists of `argument` values, namely one for each sentence.\n",
        "\n",
        "Next, print the words and their argument labels for the first 20 sentences of the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onp3jN-7YdE7",
        "outputId": "b90aa211-5bea-44a8-cbdb-96c5b8de4e09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "form: What            argument: _\n",
            "form: if              argument: _\n",
            "form: Google          argument: ARG1\n",
            "form: Morphed         argument: _\n",
            "form: Into            argument: _\n",
            "form: GoogleOS        argument: ARG2\n",
            "form: ?               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: Morphed         argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: What            argument: _\n",
            "form: if              argument: _\n",
            "form: Google          argument: ARG0\n",
            "form: expanded        argument: _\n",
            "form: on              argument: _\n",
            "form: its             argument: _\n",
            "form: search          argument: _\n",
            "form: -               argument: _\n",
            "form: engine          argument: _\n",
            "form: (               argument: _\n",
            "form: and             argument: _\n",
            "form: now             argument: _\n",
            "form: e-mail          argument: _\n",
            "form: )               argument: _\n",
            "form: wares           argument: ARG1\n",
            "form: into            argument: _\n",
            "form: a               argument: _\n",
            "form: full            argument: _\n",
            "form: -               argument: _\n",
            "form: fledged         argument: _\n",
            "form: operating       argument: _\n",
            "form: system          argument: ARG4\n",
            "form: ?               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: expanded        argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: (               argument: _\n",
            "form: And             argument: _\n",
            "form: ,               argument: _\n",
            "form: by              argument: _\n",
            "form: the             argument: _\n",
            "form: way             argument: ARGM-DIS\n",
            "form: ,               argument: _\n",
            "form: is              argument: _\n",
            "form: anybody         argument: ARG1\n",
            "form: else            argument: _\n",
            "form: just            argument: _\n",
            "form: a               argument: _\n",
            "form: little          argument: _\n",
            "form: nostalgic       argument: ARG2\n",
            "form: for             argument: _\n",
            "form: the             argument: _\n",
            "form: days            argument: _\n",
            "form: when            argument: _\n",
            "form: that            argument: _\n",
            "form: was             argument: _\n",
            "form: a               argument: _\n",
            "form: good            argument: _\n",
            "form: thing           argument: _\n",
            "form: ?               argument: _\n",
            "form: )               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: is              argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: (               argument: _\n",
            "form: And             argument: _\n",
            "form: ,               argument: _\n",
            "form: by              argument: _\n",
            "form: the             argument: _\n",
            "form: way             argument: _\n",
            "form: ,               argument: _\n",
            "form: is              argument: _\n",
            "form: anybody         argument: _\n",
            "form: else            argument: _\n",
            "form: just            argument: _\n",
            "form: a               argument: _\n",
            "form: little          argument: _\n",
            "form: nostalgic       argument: _\n",
            "form: for             argument: _\n",
            "form: the             argument: _\n",
            "form: days            argument: ARGM-TMP\n",
            "form: when            argument: R-ARGM-TMP\n",
            "form: that            argument: ARG1\n",
            "form: was             argument: _\n",
            "form: a               argument: _\n",
            "form: good            argument: _\n",
            "form: thing           argument: ARG2\n",
            "form: ?               argument: _\n",
            "form: )               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: is              argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: This            argument: _\n",
            "form: BuzzMachine     argument: ARG2\n",
            "form: post            argument: _\n",
            "form: argues          argument: _\n",
            "form: that            argument: _\n",
            "form: Google          argument: _\n",
            "form: 's              argument: _\n",
            "form: rush            argument: _\n",
            "form: toward          argument: _\n",
            "form: ubiquity        argument: _\n",
            "form: might           argument: _\n",
            "form: backfire        argument: _\n",
            "form: --              argument: _\n",
            "form: which           argument: _\n",
            "form: we              argument: _\n",
            "form: 've             argument: _\n",
            "form: all             argument: _\n",
            "form: heard           argument: _\n",
            "form: before          argument: _\n",
            "form: ,               argument: _\n",
            "form: but             argument: _\n",
            "form: it              argument: _\n",
            "form: 's              argument: _\n",
            "form: particularly    argument: _\n",
            "form: well            argument: _\n",
            "form: -               argument: _\n",
            "form: put             argument: _\n",
            "form: in              argument: _\n",
            "form: this            argument: _\n",
            "form: post            argument: _\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: post            argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: This            argument: _\n",
            "form: BuzzMachine     argument: _\n",
            "form: post            argument: ARG0\n",
            "form: argues          argument: _\n",
            "form: that            argument: _\n",
            "form: Google          argument: _\n",
            "form: 's              argument: _\n",
            "form: rush            argument: _\n",
            "form: toward          argument: _\n",
            "form: ubiquity        argument: _\n",
            "form: might           argument: _\n",
            "form: backfire        argument: ARG1\n",
            "form: --              argument: _\n",
            "form: which           argument: _\n",
            "form: we              argument: _\n",
            "form: 've             argument: _\n",
            "form: all             argument: _\n",
            "form: heard           argument: _\n",
            "form: before          argument: _\n",
            "form: ,               argument: _\n",
            "form: but             argument: _\n",
            "form: it              argument: _\n",
            "form: 's              argument: _\n",
            "form: particularly    argument: _\n",
            "form: well            argument: _\n",
            "form: -               argument: _\n",
            "form: put             argument: _\n",
            "form: in              argument: _\n",
            "form: this            argument: _\n",
            "form: post            argument: _\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: argues          argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: This            argument: _\n",
            "form: BuzzMachine     argument: _\n",
            "form: post            argument: _\n",
            "form: argues          argument: _\n",
            "form: that            argument: _\n",
            "form: Google          argument: ARG1\n",
            "form: 's              argument: _\n",
            "form: rush            argument: _\n",
            "form: toward          argument: _\n",
            "form: ubiquity        argument: ARG2\n",
            "form: might           argument: _\n",
            "form: backfire        argument: _\n",
            "form: --              argument: _\n",
            "form: which           argument: _\n",
            "form: we              argument: _\n",
            "form: 've             argument: _\n",
            "form: all             argument: _\n",
            "form: heard           argument: _\n",
            "form: before          argument: _\n",
            "form: ,               argument: _\n",
            "form: but             argument: _\n",
            "form: it              argument: _\n",
            "form: 's              argument: _\n",
            "form: particularly    argument: _\n",
            "form: well            argument: _\n",
            "form: -               argument: _\n",
            "form: put             argument: _\n",
            "form: in              argument: _\n",
            "form: this            argument: _\n",
            "form: post            argument: _\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: rush            argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: This            argument: _\n",
            "form: BuzzMachine     argument: _\n",
            "form: post            argument: _\n",
            "form: argues          argument: _\n",
            "form: that            argument: _\n",
            "form: Google          argument: _\n",
            "form: 's              argument: _\n",
            "form: rush            argument: ARG1\n",
            "form: toward          argument: _\n",
            "form: ubiquity        argument: _\n",
            "form: might           argument: ARGM-MOD\n",
            "form: backfire        argument: _\n",
            "form: --              argument: _\n",
            "form: which           argument: _\n",
            "form: we              argument: _\n",
            "form: 've             argument: _\n",
            "form: all             argument: _\n",
            "form: heard           argument: ARGM-ADV\n",
            "form: before          argument: _\n",
            "form: ,               argument: _\n",
            "form: but             argument: _\n",
            "form: it              argument: _\n",
            "form: 's              argument: _\n",
            "form: particularly    argument: _\n",
            "form: well            argument: _\n",
            "form: -               argument: _\n",
            "form: put             argument: _\n",
            "form: in              argument: _\n",
            "form: this            argument: _\n",
            "form: post            argument: _\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: backfire        argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: This            argument: _\n",
            "form: BuzzMachine     argument: _\n",
            "form: post            argument: _\n",
            "form: argues          argument: _\n",
            "form: that            argument: _\n",
            "form: Google          argument: _\n",
            "form: 's              argument: _\n",
            "form: rush            argument: _\n",
            "form: toward          argument: _\n",
            "form: ubiquity        argument: _\n",
            "form: might           argument: _\n",
            "form: backfire        argument: _\n",
            "form: --              argument: _\n",
            "form: which           argument: ARG1\n",
            "form: we              argument: ARG0\n",
            "form: 've             argument: _\n",
            "form: all             argument: ARGM-ADV\n",
            "form: heard           argument: _\n",
            "form: before          argument: ARGM-TMP\n",
            "form: ,               argument: _\n",
            "form: but             argument: _\n",
            "form: it              argument: _\n",
            "form: 's              argument: _\n",
            "form: particularly    argument: _\n",
            "form: well            argument: _\n",
            "form: -               argument: _\n",
            "form: put             argument: _\n",
            "form: in              argument: _\n",
            "form: this            argument: _\n",
            "form: post            argument: _\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: heard           argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: This            argument: _\n",
            "form: BuzzMachine     argument: _\n",
            "form: post            argument: _\n",
            "form: argues          argument: _\n",
            "form: that            argument: _\n",
            "form: Google          argument: _\n",
            "form: 's              argument: _\n",
            "form: rush            argument: _\n",
            "form: toward          argument: _\n",
            "form: ubiquity        argument: _\n",
            "form: might           argument: _\n",
            "form: backfire        argument: _\n",
            "form: --              argument: _\n",
            "form: which           argument: _\n",
            "form: we              argument: _\n",
            "form: 've             argument: _\n",
            "form: all             argument: _\n",
            "form: heard           argument: _\n",
            "form: before          argument: _\n",
            "form: ,               argument: _\n",
            "form: but             argument: _\n",
            "form: it              argument: ARG1\n",
            "form: 's              argument: _\n",
            "form: particularly    argument: ARGM-ADV\n",
            "form: well            argument: _\n",
            "form: -               argument: _\n",
            "form: put             argument: ARG2\n",
            "form: in              argument: _\n",
            "form: this            argument: _\n",
            "form: post            argument: ARGM-LOC\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: 's              argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: Google          argument: ARG1\n",
            "form: is              argument: _\n",
            "form: a               argument: _\n",
            "form: nice            argument: _\n",
            "form: search          argument: _\n",
            "form: engine          argument: ARG2\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: is              argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: Does            argument: _\n",
            "form: anybody         argument: ARG0\n",
            "form: use             argument: _\n",
            "form: it              argument: ARG1\n",
            "form: for             argument: _\n",
            "form: anything        argument: ARG2\n",
            "form: else            argument: _\n",
            "form: ?               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: use             argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: They            argument: ARG0\n",
            "form: own             argument: _\n",
            "form: blogger         argument: ARG1\n",
            "form: ,               argument: _\n",
            "form: of              argument: ARGM-ADV\n",
            "form: course          argument: _\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: own             argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: Is              argument: _\n",
            "form: that            argument: ARG1\n",
            "form: a               argument: _\n",
            "form: money           argument: _\n",
            "form: maker           argument: ARG2\n",
            "form: ?               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: Is              argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: I               argument: ARG1\n",
            "form: 'm              argument: _\n",
            "form: staying         argument: _\n",
            "form: away            argument: ARG3\n",
            "form: from            argument: _\n",
            "form: the             argument: _\n",
            "form: stock           argument: _\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: staying         argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: I               argument: ARG0\n",
            "form: doubt           argument: _\n",
            "form: the             argument: _\n",
            "form: very            argument: _\n",
            "form: few             argument: _\n",
            "form: who             argument: _\n",
            "form: actually        argument: _\n",
            "form: read            argument: _\n",
            "form: my              argument: _\n",
            "form: blog            argument: _\n",
            "form: have            argument: _\n",
            "form: not             argument: _\n",
            "form: come            argument: ARG1\n",
            "form: across          argument: _\n",
            "form: this            argument: _\n",
            "form: yet             argument: _\n",
            "form: ,               argument: _\n",
            "form: but             argument: _\n",
            "form: I               argument: _\n",
            "form: figured         argument: _\n",
            "form: I               argument: _\n",
            "form: would           argument: _\n",
            "form: put             argument: _\n",
            "form: it              argument: _\n",
            "form: out             argument: _\n",
            "form: there           argument: _\n",
            "form: anyways         argument: _\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: doubt           argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: I               argument: _\n",
            "form: doubt           argument: _\n",
            "form: the             argument: _\n",
            "form: very            argument: _\n",
            "form: few             argument: ARG0\n",
            "form: who             argument: R-ARG0\n",
            "form: actually        argument: ARGM-ADV\n",
            "form: read            argument: _\n",
            "form: my              argument: _\n",
            "form: blog            argument: ARG1\n",
            "form: have            argument: _\n",
            "form: not             argument: _\n",
            "form: come            argument: _\n",
            "form: across          argument: _\n",
            "form: this            argument: _\n",
            "form: yet             argument: _\n",
            "form: ,               argument: _\n",
            "form: but             argument: _\n",
            "form: I               argument: _\n",
            "form: figured         argument: _\n",
            "form: I               argument: _\n",
            "form: would           argument: _\n",
            "form: put             argument: _\n",
            "form: it              argument: _\n",
            "form: out             argument: _\n",
            "form: there           argument: _\n",
            "form: anyways         argument: _\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: read            argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: I               argument: _\n",
            "form: doubt           argument: _\n",
            "form: the             argument: _\n",
            "form: very            argument: _\n",
            "form: few             argument: ARG0\n",
            "form: who             argument: _\n",
            "form: actually        argument: _\n",
            "form: read            argument: _\n",
            "form: my              argument: _\n",
            "form: blog            argument: _\n",
            "form: have            argument: _\n",
            "form: not             argument: ARGM-NEG\n",
            "form: come            argument: _\n",
            "form: across          argument: _\n",
            "form: this            argument: ARG1\n",
            "form: yet             argument: ARGM-TMP\n",
            "form: ,               argument: _\n",
            "form: but             argument: _\n",
            "form: I               argument: _\n",
            "form: figured         argument: _\n",
            "form: I               argument: _\n",
            "form: would           argument: _\n",
            "form: put             argument: _\n",
            "form: it              argument: _\n",
            "form: out             argument: _\n",
            "form: there           argument: _\n",
            "form: anyways         argument: _\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: come            argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: I               argument: _\n",
            "form: doubt           argument: _\n",
            "form: the             argument: _\n",
            "form: very            argument: _\n",
            "form: few             argument: _\n",
            "form: who             argument: _\n",
            "form: actually        argument: _\n",
            "form: read            argument: _\n",
            "form: my              argument: _\n",
            "form: blog            argument: _\n",
            "form: have            argument: _\n",
            "form: not             argument: _\n",
            "form: come            argument: _\n",
            "form: across          argument: _\n",
            "form: this            argument: _\n",
            "form: yet             argument: _\n",
            "form: ,               argument: _\n",
            "form: but             argument: _\n",
            "form: I               argument: ARG0\n",
            "form: figured         argument: _\n",
            "form: I               argument: _\n",
            "form: would           argument: _\n",
            "form: put             argument: ARG1\n",
            "form: it              argument: _\n",
            "form: out             argument: _\n",
            "form: there           argument: _\n",
            "form: anyways         argument: _\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: figured         argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: I               argument: _\n",
            "form: doubt           argument: _\n",
            "form: the             argument: _\n",
            "form: very            argument: _\n",
            "form: few             argument: _\n",
            "form: who             argument: _\n",
            "form: actually        argument: _\n",
            "form: read            argument: _\n",
            "form: my              argument: _\n",
            "form: blog            argument: _\n",
            "form: have            argument: _\n",
            "form: not             argument: _\n",
            "form: come            argument: _\n",
            "form: across          argument: _\n",
            "form: this            argument: _\n",
            "form: yet             argument: _\n",
            "form: ,               argument: _\n",
            "form: but             argument: _\n",
            "form: I               argument: _\n",
            "form: figured         argument: _\n",
            "form: I               argument: ARG0\n",
            "form: would           argument: ARGM-MOD\n",
            "form: put             argument: _\n",
            "form: it              argument: ARG1\n",
            "form: out             argument: _\n",
            "form: there           argument: ARG2\n",
            "form: anyways         argument: ARGM-ADV\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: put             argument: None\n",
            "\n",
            "========================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_sentences(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, the sequence of word forms runs parallel to the sequence of argument labels. This means that for every index of `input_form`, the same index of `argument` gives its argument label.\n",
        "\n",
        "Argument labels are:\n",
        "- **'_'** for tokens that are not an argument (in the current predicate sense of the sentence).\n",
        "- The token's respective Propbank label for tokens that are an argument, e.g. **ARG1**\n",
        "- **None** for the special separator token (`[SEP]`) and the predicate token that follows the separator.\n",
        "\n",
        "For example, in the the first sentence of the test data printed above (\"What if Google Morphed Into GoogleOS?\"), the predicate 'Morphed' evokes [the frame `morph.01`](https://propbank.github.io/v3.4.0/frames/morph.html#morph.01). The frame's arguments are:\n",
        "\n",
        "- `ARG0-PAG`: causer of transformation\n",
        "- `ARG1-PPT`: thing changing\n",
        "- `ARG2-PRD`: end state\n",
        "- `ARG3-VSP`: start state\n",
        "\n",
        "In this example, the `ARG1` label is assigned to 'Google', and the `ARG2` label is assigned to 'GoogleOS', which indicates 'Google' is the thing that is changing and 'GoogleOS' is its end state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSXRRKutYdE7"
      },
      "source": [
        "## Step 2: Tokenize the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5g8f-8dYdE8"
      },
      "source": [
        "Now that you have extracted sentences and labels from the datasets, you need to prepare the sentences for the BERT model by tokenizing them.\n",
        "\n",
        "Use HuggingFace's [`AutoTokenizer`](https://huggingface.co/docs/transformers/v4.38.2/en/model_doc/auto#transformers.AutoTokenizer) to construct a DistilBERT tokenizer, which is based on the WordPiece algorithm. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YJxQoz_GYdE8"
      },
      "outputs": [],
      "source": [
        "# Set the model ID to use\n",
        "model_id = \"distilbert-base-uncased\"\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Check the assertion that the tokenizer is an instance of transformers.PreTrainedTokenizerFast\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi0vG2xMYdE8"
      },
      "source": [
        "To test the `tokenizer()`, tokenize the first sentence of the test data, including:\n",
        "\n",
        "- `add_special_tokens` set to **True** to add a `[CLS]` token to the start of every sentence.\n",
        "- `is_split_into_words` set to **True** because the sentence is already split into words (based on the Universal Propbank 1.0 dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Wg7_MEckUoi",
        "outputId": "c4457c76-b7e0-4299-b4c5-3a71274fe3cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     [CLS] 101\n",
            "      what 2054\n",
            "        if 2065\n",
            "    google 8224\n",
            "       mor 22822\n",
            "      ##ph 8458\n",
            "      ##ed 2098\n",
            "      into 2046\n",
            "    google 8224\n",
            "      ##os 2891\n",
            "         ? 1029\n",
            "     [SEP] 102\n",
            "       mor 22822\n",
            "      ##ph 8458\n",
            "      ##ed 2098\n",
            "     [SEP] 102\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the first example in the test data\n",
        "example = test_data['input_form'][0]\n",
        "tokenized_input = tokenizer(example,add_special_tokens=True, is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "\n",
        "# Print the example tokens and their corresponding IDs\n",
        "for token, id in zip(tokens, tokenized_input[\"input_ids\"]):\n",
        "    print(f\"{token:>10} {id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You've successfully tokenized the sample sentence, splitting words up into subword tokens and fetching their token IDs from DistilBERT's vocabulary.\n",
        "\n",
        "> Note: notice how the special tokens `[CLS]` and `[SEP]` are tokenized as **101** and **102**. These numbers are meaningful to BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lERE6j_HYdE-"
      },
      "source": [
        "## Step 3: Prepare the input for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8BO8OyoYdE-"
      },
      "source": [
        "Before training the model, map the labels in the datasets to numerical values. This ensures consistency and facilitates the training process.\n",
        "\n",
        "To get the label mapping, call `get_label_mapping()`, including:\n",
        "\n",
        "| Parameter name     | Required | Parameter description |\n",
        "|--------------------|:--------------:|-------------|\n",
        "| *positional 1* (DataFrame)          | ✅️ | The training dataset for which to extract the label mapping. |\n",
        "| *positional 2* (DataFrame)          | ✅ | The test dataset for which to extract the label mapping.|\n",
        "| *positional 3* (DataFrame)          | ✅ | The dev dataset for which to extract the label mapping. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kangOKNiYdE-"
      },
      "outputs": [],
      "source": [
        "label_map = get_label_mapping(train_data, test_data, dev_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `get_label_mapping()` function returns an alphabetically-ordered dictionary mapping:\n",
        "- **_** to **0**.\n",
        "- String labels to integers, e.g. **ARG0** to **1**.\n",
        "- **None** to **None**, to preserve the labels for special tokens and predicates. (You will replace **None** with **-100** later to mask these tokens from being labeled.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id6U73tRYdE_",
        "outputId": "61c536d9-1941-4d09-d9f1-858209868edd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'_': 0, 'ARG0': 1, 'ARG1': 2, 'ARG1-DSP': 3, 'ARG2': 4, 'ARG3': 5, 'ARG4': 6, 'ARG5': 7, 'ARGA': 8, 'ARGM-ADJ': 9, 'ARGM-ADV': 10, 'ARGM-CAU': 11, 'ARGM-COM': 12, 'ARGM-CXN': 13, 'ARGM-DIR': 14, 'ARGM-DIS': 15, 'ARGM-EXT': 16, 'ARGM-GOL': 17, 'ARGM-LOC': 18, 'ARGM-LVB': 19, 'ARGM-MNR': 20, 'ARGM-MOD': 21, 'ARGM-NEG': 22, 'ARGM-PRD': 23, 'ARGM-PRP': 24, 'ARGM-PRR': 25, 'ARGM-REC': 26, 'ARGM-TMP': 27, 'C-ARG0': 28, 'C-ARG1': 29, 'C-ARG1-DSP': 30, 'C-ARG2': 31, 'C-ARG3': 32, 'C-ARG4': 33, 'C-ARGM-ADV': 34, 'C-ARGM-COM': 35, 'C-ARGM-CXN': 36, 'C-ARGM-DIR': 37, 'C-ARGM-EXT': 38, 'C-ARGM-GOL': 39, 'C-ARGM-LOC': 40, 'C-ARGM-MNR': 41, 'C-ARGM-PRP': 42, 'C-ARGM-PRR': 43, 'C-ARGM-TMP': 44, 'R-ARG0': 45, 'R-ARG1': 46, 'R-ARG2': 47, 'R-ARG3': 48, 'R-ARG4': 49, 'R-ARGM-ADJ': 50, 'R-ARGM-ADV': 51, 'R-ARGM-CAU': 52, 'R-ARGM-COM': 53, 'R-ARGM-DIR': 54, 'R-ARGM-GOL': 55, 'R-ARGM-LOC': 56, 'R-ARGM-MNR': 57, 'R-ARGM-TMP': 58, None: None}\n"
          ]
        }
      ],
      "source": [
        "print(label_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiyFPfnuYdFA"
      },
      "source": [
        "\n",
        "Next, apply the label mapping to the datasets, adding the column `mapped_labels` to the DataFrames. This column contains arrays of integers representing the labels, based on the label mapping. \n",
        "\n",
        "To apply the label mapping, call `map_labels_in_dataframe()`, including:\n",
        "\n",
        "| Parameter name     | Required | Parameter description |\n",
        "|--------------------|:--------------:|-------------|\n",
        "| *positional 1*                   | ✅️ | The DataFrame for which to convert the argument labels. |\n",
        "| *positional 2*                 | ✅ | The label mapping, created with `get_label_mapping()`. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "N1Cy6U0nYdFB"
      },
      "outputs": [],
      "source": [
        "train_data = map_labels_in_dataframe(train_data, label_map)\n",
        "dev_data = map_labels_in_dataframe(dev_data, label_map)\n",
        "test_data = map_labels_in_dataframe(test_data, label_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1MzLjuoYdFB"
      },
      "source": [
        "As you can see, for each row in the DataFrame, the values in `mapped_labels` and `arguments` correspond to the mapping in `label_map`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "NrsuWwugYdFB",
        "outputId": "2295307d-bdbb-4b21-fa20-2060beeb23d3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input_form</th>\n",
              "      <th>argument</th>\n",
              "      <th>mapped_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[What, if, Google, Morphed, Into, GoogleOS, ?,...</td>\n",
              "      <td>[_, _, ARG1, _, _, ARG2, _, None, None]</td>\n",
              "      <td>[0, 0, 2, 0, 0, 4, 0, None, None]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[What, if, Google, expanded, on, its, search, ...</td>\n",
              "      <td>[_, _, ARG0, _, _, _, _, _, _, _, _, _, _, _, ...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[(, And, ,, by, the, way, ,, is, anybody, else...</td>\n",
              "      <td>[_, _, _, _, _, ARGM-DIS, _, _, ARG1, _, _, _,...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 15, 0, 0, 2, 0, 0, 0, 0, 4, 0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[(, And, ,, by, the, way, ,, is, anybody, else...</td>\n",
              "      <td>[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[This, BuzzMachine, post, argues, that, Google...</td>\n",
              "      <td>[_, ARG2, _, _, _, _, _, _, _, _, _, _, _, _, ...</td>\n",
              "      <td>[0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          input_form  \\\n",
              "0  [What, if, Google, Morphed, Into, GoogleOS, ?,...   \n",
              "1  [What, if, Google, expanded, on, its, search, ...   \n",
              "2  [(, And, ,, by, the, way, ,, is, anybody, else...   \n",
              "3  [(, And, ,, by, the, way, ,, is, anybody, else...   \n",
              "4  [This, BuzzMachine, post, argues, that, Google...   \n",
              "\n",
              "                                            argument  \\\n",
              "0            [_, _, ARG1, _, _, ARG2, _, None, None]   \n",
              "1  [_, _, ARG0, _, _, _, _, _, _, _, _, _, _, _, ...   \n",
              "2  [_, _, _, _, _, ARGM-DIS, _, _, ARG1, _, _, _,...   \n",
              "3  [_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, ...   \n",
              "4  [_, ARG2, _, _, _, _, _, _, _, _, _, _, _, _, ...   \n",
              "\n",
              "                                       mapped_labels  \n",
              "0                  [0, 0, 2, 0, 0, 4, 0, None, None]  \n",
              "1  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...  \n",
              "2  [0, 0, 0, 0, 0, 15, 0, 0, 2, 0, 0, 0, 0, 4, 0,...  \n",
              "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "4  [0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78d8pDrEYdFB"
      },
      "source": [
        "Now that you have initialized and tested the `tokenizer()` and added mapped labels to the DataFrames, it's time to tokenize (and pad) all sentences. \n",
        "\n",
        "Since WordPiece tokenization potentially breaks words up into subword tokens, the tokens and their labels have to be re-aligned. The `tokenize_and_align_labels()` function you'll call for this iterates over each token and determines the appropriate label based on the provided dataset. \n",
        "\n",
        "Special tokens are assigned a label of **-100** to indicate they should be ignored in the loss function. Labels for the first token of each word are set accordingly, while labels for subsequent tokens within the same word are determined based on the `label_all_tokens` flag.\n",
        "\n",
        "To tokenize the sentences and align the labels, call `tokenize_and_align_labels()`, including:\n",
        "\n",
        "| Parameter name     | Required | Parameter description |\n",
        "|--------------------|:--------------:|-------------|\n",
        "| *positional 1* (`transformers AutoTokenizer`) | ✅️ | The `tokenizer()` for the pre-trained model. |\n",
        "| *positional 2* (DataFrame) | ✅ | The preprocessed datasets |\n",
        "| `label_all_tokens` (boolean)     | Optional (defaults to **True**) | Whether all tokens should receive their own label, accounting for words split into subtokens |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OHRNa98wYdFB"
      },
      "outputs": [],
      "source": [
        "tokenized_test = tokenize_and_align_labels(tokenizer, test_data, label_all_tokens=True)\n",
        "tokenized_train = tokenize_and_align_labels(tokenizer, train_data, label_all_tokens=True)\n",
        "tokenized_dev = tokenize_and_align_labels(tokenizer, dev_data, label_all_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9WeeG8YYdFB"
      },
      "source": [
        "Now that you have tokenized all three datasets, let's examine the result. \n",
        "\n",
        "The `tokenized_` datasets are of the type `transformers.tokenization_utils_base.BatchEncoding` and have three attributes per row:\n",
        "\n",
        "1. `input_ids`: an array of token IDs for the tokenized sentence. Starts with the token ID for the `[CLS]` token, followed by the tokenized sentence, the `[SEP]` token, the predicate, and a final `[SEP]` token. \n",
        "2. `attention_mask`: an array representing the attention mask for the sentence.\n",
        "3. `labels`: an array with numerical labels, aligned with the tokens. \n",
        "\n",
        "> Note: all three arrays are padded so that every sample per dataset is of equal length.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e23nB-4dYdFB",
        "outputId": "f6e786d8-61b4-405a-cbac-412771ebf92e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
            "['[CLS]', 'what', 'if', 'google', 'mor', '##ph', '##ed', 'into', 'google', '##os', '?', '[SEP]', 'mor', '##ph', '##ed', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "input_ids: tensor([  101,  2054,  2065,  8224, 22822,  8458,  2098,  2046,  8224,  2891,\n",
            "         1029,   102, 22822,  8458,  2098,   102,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0])\n",
            "attention_mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0])\n",
            "labels: [-100, 0, 0, 2, 0, 0, 0, 0, 4, 4, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"
          ]
        }
      ],
      "source": [
        "print(type(tokenized_test))\n",
        "print(tokenized_test.keys())\n",
        "print(tokenizer.convert_ids_to_tokens(tokenized_test[\"input_ids\"][0]))\n",
        "for key in tokenized_test.keys():\n",
        "    print(f\"{key}: {tokenized_test[key][0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j31tpMcxYdFB"
      },
      "source": [
        "To confirm that we have padded all sentences in the `tokenized_test` dataset to be of equal length, let's check the length of all three arrays for the first 10 sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp6JKWx6YdFC",
        "outputId": "baac2bc5-bbdb-4dcf-d708-464718dcaa0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentence 0: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
            "sentence 1: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
            "sentence 2: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
            "sentence 3: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
            "sentence 4: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
            "sentence 5: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
            "sentence 6: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
            "sentence 7: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
            "sentence 8: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
            "sentence 9: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    print(f\"sentence {i}:\", \"input_ids:\", len(tokenized_test[\"input_ids\"][i]), \"\\tlabels:\", len(tokenized_test[\"labels\"][i]), \"\\tattention_mask:\", len(tokenized_test[\"attention_mask\"][i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uui9puOfYdFC"
      },
      "source": [
        "Converting the tokenized data to datasets format with the function `load_dataset`\n",
        "\n",
        "Now that you have tokenized and padded the sentences, and aligned the labels with the tokens, you're ready to transform the tokenized datasets into Hugging Face's [`datasets.arrow_dataset.Dataset`](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset).\n",
        "\n",
        "To transform the tokenized datasets into `Dataset` objects, call the `load_dataset()` function, which calls the [`Dataset.from_dict()` method](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.from_dict), including:\n",
        "\n",
        "| Parameter name     | Required | Parameter description |\n",
        "|--------------------|:--------------:|-------------|\n",
        "| *positional 1* (`transformers.tokenization_utils_base.BatchEncoding`) | ✅️ | The tokenized dataset. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kBKxTmz3YdFC"
      },
      "outputs": [],
      "source": [
        "dataset_train = load_dataset(tokenized_train)\n",
        "dataset_dev = load_dataset(tokenized_dev)\n",
        "dataset_test = load_dataset(tokenized_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's print the type of the resulting dataset, to confirm the transformation into `datasets.arrow_dataset.Dataset`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'datasets.arrow_dataset.Dataset'>\n"
          ]
        }
      ],
      "source": [
        "print(type(dataset_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-Pl5GvcYdFC"
      },
      "source": [
        "## Step 4: Fine-tune the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5yTu8JzYdFC"
      },
      "source": [
        "Finally, the sentences have been transformed from CoNNL-U Plus format to Hugging Face `Dataset` objects: it's time to fine-tune BERT!\n",
        "\n",
        "Fine-tuning a BERT model on the full dataset can be a very computationally challenging task. To speed up the process, create subsets of the three datasets with 1000 samples per dataset, selected randomly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SLNvuvOvYdFD"
      },
      "outputs": [],
      "source": [
        "#small_train_dataset = dataset_train.shuffle(seed=42).select(range(1000))\n",
        "#small_eval_dataset = dataset_dev.shuffle(seed=42).select(range(1000))\n",
        "#small_test_dataset = dataset_test.shuffle(seed=42).select(range(1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BzGFWMKYdFE"
      },
      "source": [
        "To map the numerical labels back to their string representations, you need to convert the `label_map` dictionary to a list of labels (as strings).\n",
        "\n",
        "To convert the `label_map` to a list of labels (as strings), call the `get_labels_from_map()` function, including:\n",
        "\n",
        "| Parameter name     | Required | Parameter description |\n",
        "|--------------------|:--------------:|-------------|\n",
        "| *positional 1* (dictionary) | ✅️ | The dictionary mapping labels as strings to their numerical represenation. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qGc0sBr1YdFE"
      },
      "outputs": [],
      "source": [
        "label_list = get_labels_from_map(label_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCsgWLXQYdFE"
      },
      "source": [
        "Next, load the pretrained DistilBERT model using the [`AutoModelForTokenClassification.from_pretrained()` method](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained) from the `transformers` library, together with the model name (**distilbert-base-uncased**), and the [`TrainingArguments`](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/trainer#transformers.TrainingArguments) neccesary for training.\n",
        "\n",
        "To get the model, model name and `TrainingArguments`, call the `load_srl_model()` function, including:\n",
        "\n",
        "| Parameter name     | Required | Parameter description |\n",
        "|--------------------|:--------------:|-------------|\n",
        "| *positional 1* (string) | ✅️ | The model identifier.  |\n",
        "| *positional 2* (list of strings) | ✅️ | The tokenized dataset. |\n",
        "| `batch_size` (integer) | Optional (defaults to **16**) | The [batch size for training](https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#methods-and-tools-for-efficient-training-on-a-single-gpu) and [inference](https://huggingface.co/docs/setfit/main/en/how_to/batch_sizes). |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "679ec2c3ea6d4b34b037d03f3191e47b",
            "396674e8ce914ef0993ee401ca6a748b",
            "887ed7ab216b47c2aeea6ee5480b1f45",
            "f1e1fd56e6d04450b389568d13d2f004",
            "e073ccca16a541d1bb8f60105140a924",
            "b433ada7b8644d738e2946241fdd5537",
            "fac319d2c2a048dd96cdc0194d1834ee",
            "7e2d89cc14a44d43b51916619bdda029",
            "6c9e88a2e72046c5b475e02ee943ba2e",
            "73efdd763f944e459885fb3a6c37a7dc",
            "92217122f70c4873a9ad3caab5014f24"
          ]
        },
        "id": "t8OACqyhYdFF",
        "outputId": "1dad5a07-5e2a-4ab9-de10-08b51e8999ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model, args = load_srl_model(model_id, label_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFTYKnm1YdFF"
      },
      "source": [
        "Now that you have a DistilBERT model, it's time for fine-tuning the model for the task of semantic role labeling (SRL).\n",
        "\n",
        "To fine-tune your model, instantiate a [`Trainer` object](https://huggingface.co/docs/transformers/main_classes/trainer#api-reference%20][%20transformers.Trainer) from the `transformers` library, passing the `model`, `args`, `tokenizer` and datasets for training and inference. Then, call the [`Trainer.train()` method](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.train) to start the fine-tuning process.\n",
        "\n",
        "> Note: this process may take up to several hours, depending on your hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "tZVjfPfuYdFF",
        "outputId": "5fe05151-c90a-4c6c-db37-a833aa6bd04e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kris/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 33521\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 6288\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='6288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   3/6288 00:11 < 19:19:32, 0.09 it/s, Epoch 0.00/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m         model,\n\u001b[1;32m      3\u001b[0m         args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m         compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m p: compute_metrics(\u001b[38;5;241m*\u001b[39mp, label_list)\n\u001b[1;32m      8\u001b[0m     )\n\u001b[0;32m----> 9\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1365\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1363\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1365\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1368\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1370\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1371\u001b[0m ):\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1958\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1956\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1958\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "        model,\n",
        "        args,\n",
        "        train_dataset=dataset_train,\n",
        "        eval_dataset=dataset_dev,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=lambda p: compute_metrics(*p, label_list)\n",
        "    )\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXZ8ro_iYdFF"
      },
      "source": [
        "Now that you have fine-tuned the model, let's evaluate its performance on the `eval_dataset` that you set when constructing the `Trainer` instance. \n",
        "\n",
        "To evaluate the fine-tuned model, call the [`Trainer.evaluate()` method](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.evaluate)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "XUpcBt68YdFG",
        "outputId": "b6adfd7c-786f-41aa-c9e8-a209a82c7ceb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='312' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [312/312 00:13]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.12445865571498871,\n",
              " 'eval_precision': 0.4244081146244632,\n",
              " 'eval_recall': 0.3919038696272433,\n",
              " 'eval_f1': 0.39952554037676824,\n",
              " 'eval_accuracy': 0.9652914041791456,\n",
              " 'eval_runtime': 15.8633,\n",
              " 'eval_samples_per_second': 313.742,\n",
              " 'eval_steps_per_second': 19.668,\n",
              " 'epoch': 3.0}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9NFI6DmYdFG"
      },
      "source": [
        "Now that you have fine-tuned your DistilBERT model for semantic role labeling, and evaluated its performance on the development dataset, it's time to infer the argument labels of the test dataset and compute a summary of the performance metrics.\n",
        "\n",
        "First, call the [`Trainer.predict()` method](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.predict) passing the test dataset. The method returns a tuple consisting of the model's predictions on the test dataset, the labels, and metrics. \n",
        "\n",
        "To compute a summary of the model's perfomance metrics on the test dataset, call the `compute_metrics()` function, including:\n",
        "\n",
        "| Parameter name     | Required | Parameter description |\n",
        "|--------------------|:--------------:|-------------|\n",
        "| *positional 1* (`np.ndarray`) | ✅️ | The array of predictions as returned from the `Trainer.predict()` method. |\n",
        "| *positional 2* (`np.ndarray`) | ✅️ | The array of argument labels as returned from the `Trainer.predict()` method. |\n",
        "| *positional 3* (list of strings) | ✅️ | The list of argument labels as strings. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "tMNBhvOcYdFG",
        "outputId": "e8e1fdbc-9582-4e86-caff-2ddee61aebba"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'precision': 0.42951477322261544,\n",
              " 'recall': 0.4030070005307468,\n",
              " 'f1': 0.4036396071913792,\n",
              " 'accuracy': 0.9663773918794519}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions, labels, _ = trainer.predict(dataset_test)\n",
        "results = compute_metrics(predictions, labels, label_list)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrvtvLY9YdFG"
      },
      "source": [
        "Writing the predictions together with the gold labels to a csv file with the function `write_predictions_to_csv` so that the metrics per class can be computed with the `compute_evaluation_metrics_from_csv` function.\n",
        "\n",
        "Now that you have succesfully fine-tuned a model and inferred the argument labels of the test set, let's store the results on disc in a CSV file, and create a full classification report. \n",
        "\n",
        "To write the predictions to CSV, call the `write_predictions_to_csv()` function, including:\n",
        "\n",
        "| Parameter name     | Required | Parameter description |\n",
        "|--------------------|:--------------:|-------------|\n",
        "| *positional 1* (`np.ndarray`) | ✅️ | The array of predictions as returned from the `Trainer.predict()` method. |\n",
        "| *positional 2* (`np.ndarray`) | ✅️ | The array of argument labels as returned from the `Trainer.predict()` method. |\n",
        "| *positional 3* (list of strings) | ✅️ | The list of argument labels as strings. |\n",
        "| *positional 3* (string) | ✅️ | The filepath to write the results to (in CSV format). |\n",
        "\n",
        "Next, to compute a full classification report of the model's performance on the test dataset, call the `compute_evaluation_metrics_from_csv()` function, including:\n",
        "\n",
        "| Parameter name     | Required | Parameter description |\n",
        "|--------------------|:--------------:|-------------|\n",
        "| *positional 1* (string) | ✅️ | The filepath for the CSV file where the model's predictions are stored. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8J6utkmYdFG",
        "outputId": "a4aa0356-2014-411b-f995-2822ee434e6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ARG0       0.87      0.81      0.84      2023\n",
            "        ARG1       0.84      0.81      0.82      3750\n",
            "    ARG1-DSP       0.00      0.00      0.00         0\n",
            "        ARG2       0.70      0.69      0.70      1309\n",
            "        ARG3       0.01      0.25      0.02         4\n",
            "        ARG4       0.59      0.62      0.61        64\n",
            "        ARG5       0.00      0.00      0.00         0\n",
            "        ARGA       0.00      0.00      0.00         0\n",
            "    ARGM-ADJ       0.76      0.74      0.75       261\n",
            "    ARGM-ADV       0.55      0.67      0.61       429\n",
            "    ARGM-CAU       0.58      0.52      0.55        54\n",
            "    ARGM-COM       0.25      0.57      0.35         7\n",
            "    ARGM-CXN       0.50      0.75      0.60         8\n",
            "    ARGM-DIR       0.36      0.49      0.41        35\n",
            "    ARGM-DIS       0.72      0.74      0.73       191\n",
            "    ARGM-EXT       0.76      0.75      0.76       106\n",
            "    ARGM-GOL       0.07      0.67      0.12         3\n",
            "    ARGM-LOC       0.64      0.57      0.60       290\n",
            "    ARGM-LVB       0.74      0.70      0.72        73\n",
            "    ARGM-MNR       0.52      0.59      0.55       144\n",
            "    ARGM-MOD       0.94      0.89      0.91       499\n",
            "    ARGM-NEG       0.96      0.87      0.91       435\n",
            "    ARGM-PRD       0.20      0.43      0.27        23\n",
            "    ARGM-PRP       0.61      0.55      0.58        91\n",
            "    ARGM-PRR       0.65      0.67      0.66        75\n",
            "    ARGM-TMP       0.81      0.72      0.77       658\n",
            "      C-ARG0       0.00      0.00      0.00         0\n",
            "      C-ARG1       0.54      0.58      0.56        53\n",
            "  C-ARG1-DSP       0.00      0.00      0.00         0\n",
            "      C-ARG2       0.00      0.00      0.00         0\n",
            "      C-ARG3       0.00      0.00      0.00         0\n",
            "  C-ARGM-CXN       0.00      0.00      0.00         0\n",
            "  C-ARGM-LOC       0.00      0.00      0.00         0\n",
            "      R-ARG0       0.90      0.90      0.90        67\n",
            "      R-ARG1       0.83      0.64      0.72        67\n",
            "      R-ARG2       0.00      0.00      0.00         0\n",
            "  R-ARGM-ADJ       0.00      0.00      0.00         0\n",
            "  R-ARGM-ADV       0.00      0.00      0.00         0\n",
            "  R-ARGM-DIR       0.00      0.00      0.00         0\n",
            "  R-ARGM-LOC       0.44      0.29      0.35        14\n",
            "  R-ARGM-MNR       0.00      0.00      0.00         0\n",
            "  R-ARGM-TMP       0.00      0.00      0.00         0\n",
            "           _       0.99      0.99      0.99    101156\n",
            "\n",
            "    accuracy                           0.97    111889\n",
            "   macro avg       0.40      0.43      0.40    111889\n",
            "weighted avg       0.97      0.97      0.97    111889\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "results_file = \"predictions.csv\"\n",
        "write_predictions_to_csv(predictions, labels, label_list, results_file)\n",
        "classification_report = compute_evaluation_metrics_from_csv(\"predictions.csv\")\n",
        "print(classification_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rBk6GBWYdFG"
      },
      "source": [
        "Finally, store the `tokenizer`, `trainer` and `model` on disc using their built-in methods. For each method call, pass a string representing the directory to save the object and its configuration to.\n",
        "\n",
        "This let's you use the objects' built-in `from_pretrained()` methods to reload their state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Cni2yCZiYdFG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tokenizer config file saved in tokenizer.save_pretrained.distillbert-base-uncased-finetuned-srl/tokenizer_config.json\n",
            "Special tokens file saved in tokenizer.save_pretrained.distillbert-base-uncased-finetuned-srl/special_tokens_map.json\n",
            "Saving model checkpoint to trainer.save_model.distillbert-base-uncased-finetuned-srl\n",
            "Configuration saved in trainer.save_model.distillbert-base-uncased-finetuned-srl/config.json\n",
            "Model weights saved in trainer.save_model.distillbert-base-uncased-finetuned-srl/pytorch_model.bin\n",
            "tokenizer config file saved in trainer.save_model.distillbert-base-uncased-finetuned-srl/tokenizer_config.json\n",
            "Special tokens file saved in trainer.save_model.distillbert-base-uncased-finetuned-srl/special_tokens_map.json\n",
            "Configuration saved in model.save_pretrained.distillbert-base-uncased-finetuned-srl/config.json\n",
            "Model weights saved in model.save_pretrained.distillbert-base-uncased-finetuned-srl/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "# Use these codes to save model:\n",
        "tokenizer.save_pretrained(\"tokenizer.save_pretrained.distillbert-base-uncased-finetuned-srl\")\n",
        "trainer.save_model(\"trainer.save_model.distillbert-base-uncased-finetuned-srl\")\n",
        "model.save_pretrained(\"model.save_pretrained.distillbert-base-uncased-finetuned-srl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you are running this notebook in Google colab, create a directory in your Google Drive and copy the `tokenizer`, `trainer`, and `model` to your Google Drive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "avWiTIQcjubr"
      },
      "outputs": [],
      "source": [
        "if runs_in_colab:\n",
        "    !mkdir -p \"/content/drive/MyDrive/NLP_3_baseline_model/model\"\n",
        "    !cp -r '/content/trainer.save_model.distillbert-base-uncased-finetuned-srl' '/content/drive/MyDrive/NLP_3_baseline_model/model'\n",
        "    !cp -r '/content/model.save_pretrained.distillbert-base-uncased-finetuned-srl' '/content/drive/MyDrive/NLP_3_baseline_model/model'\n",
        "    !cp -r '/content/tokenizer.save_pretrained.distillbert-base-uncased-finetuned-srl' '/content/drive/MyDrive/NLP_3_baseline_model/model'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGd8F8QGYdFH"
      },
      "source": [
        "## Group Contribution:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HHdp36FYdFH"
      },
      "source": [
        "##### Ariana Britez:\n",
        "- functions to map the labels to number for model input: get_label_mapping, map_labels_to_numbers, map_labels_in_dataframe\n",
        "- function to get the list of labels for model input: get_labels_from_map\n",
        "- function to compute the metrics during training, evaluation and inference: compute_metrics, compute_evaluation_metrics_from_csv\n",
        "- function to load the transformer model for fine-tuning: load_srl_model\n",
        "- function to load the dataset in format that model can handle: load_dataset\n",
        "- function to save the model predictions with gold labels for evaluation: write_predictions_to_csv\n",
        "- writing markdown from importing the model section until evaluation of the baseline model\n",
        "\n",
        "##### Kris Stallenberg:\n",
        "- Writing tutorial-style documentation for the full pipeline.\n",
        "- Refactoring functions in the utils.py module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "396674e8ce914ef0993ee401ca6a748b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b433ada7b8644d738e2946241fdd5537",
            "placeholder": "​",
            "style": "IPY_MODEL_fac319d2c2a048dd96cdc0194d1834ee",
            "value": "model.safetensors: 100%"
          }
        },
        "679ec2c3ea6d4b34b037d03f3191e47b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_396674e8ce914ef0993ee401ca6a748b",
              "IPY_MODEL_887ed7ab216b47c2aeea6ee5480b1f45",
              "IPY_MODEL_f1e1fd56e6d04450b389568d13d2f004"
            ],
            "layout": "IPY_MODEL_e073ccca16a541d1bb8f60105140a924"
          }
        },
        "6c9e88a2e72046c5b475e02ee943ba2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "73efdd763f944e459885fb3a6c37a7dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e2d89cc14a44d43b51916619bdda029": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "887ed7ab216b47c2aeea6ee5480b1f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e2d89cc14a44d43b51916619bdda029",
            "max": 267954768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c9e88a2e72046c5b475e02ee943ba2e",
            "value": 267954768
          }
        },
        "92217122f70c4873a9ad3caab5014f24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b433ada7b8644d738e2946241fdd5537": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e073ccca16a541d1bb8f60105140a924": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1e1fd56e6d04450b389568d13d2f004": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73efdd763f944e459885fb3a6c37a7dc",
            "placeholder": "​",
            "style": "IPY_MODEL_92217122f70c4873a9ad3caab5014f24",
            "value": " 268M/268M [00:02&lt;00:00, 83.6MB/s]"
          }
        },
        "fac319d2c2a048dd96cdc0194d1834ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
