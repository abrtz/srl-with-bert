{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Semantic role labeling with BERT\n",
        "\n",
        "In this notebook, you'll perform semantic role labeling with BERT, using the [English Universal Propbank 1.0 datasets](https://github.com/UniversalPropositions/UP-1.0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Before you begin\n",
        "\n",
        "### Install libraries\n",
        "\n",
        "If you run this notebook in [Google colab](https://colab.research.google.com), make sure to install the required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    runs_in_colab =  True\n",
        "except ImportError:\n",
        "    runs_in_colab = False\n",
        "\n",
        "if runs_in_colab:\n",
        "    !pip install datasets\n",
        "    !pip install seqeval\n",
        "    !pip install accelerate==0.21.0\n",
        "    !pip install transformers[torch]\n",
        "    !pip install accelerate -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Note: If you need to install the packages locally, adjust the commands match your Python environment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paAVq25AkUoc"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "u8VbzlMekUod"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
        "from datasets import load_metric\n",
        "from datasets import Dataset\n",
        "from utils import read_data_as_sentence,map_labels_in_dataframe,tokenize_and_align_labels,get_label_mapping,get_labels_from_map,load_srl_model,load_dataset,compute_metrics,write_predictions_to_csv,compute_evaluation_metrics_from_csv, print_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPqKu0URkUoe"
      },
      "source": [
        "## Step 1: Preprocess data\n",
        "\n",
        "Unlike traditional token labeling methods, which assign labels to individual words in isolation, BERT performs sequence labeling. This means BERT assigns labels to individual tokens, while taking the full sentence context in consideration. \n",
        "\n",
        "The English Universal PropBank 1.0 dataset is structured in [CoNNL-U Plus format](https://universaldependencies.org/ext-format.html), in which lines represent individual tokens. So before you can train the model, you need to extract sentences and labels from the datasets, and preprocess the sentences by removing non-argument labels.\n",
        "\n",
        "To preprocess the datasets and save the resulting DataFrame to a file, call the `read_data_as_sentence()` function, including:\n",
        "\n",
        "| Parameter name     | Required | Parameter description |\n",
        "|--------------------|:--------------:|-------------|\n",
        "| *positional 1*  (string)                 | ✅️ | The filepath for the CoNNLU dataset. |\n",
        "| *positional 2*  (string)               | ✅ | The filepath to write the preprocessed DataFrame to. |\n",
        "| `mode` (enum)                          | Optional (defaults to **basic**) | The method of preprocessing, used for the advanced model. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "oDHf8YvpkUof"
      },
      "outputs": [],
      "source": [
        "train_data = read_data_as_sentence('data/en_ewt-up-train.conllu', 'data/en_ewt-up-train.preprocessed.csv')\n",
        "dev_data = read_data_as_sentence('data/en_ewt-up-dev.conllu', 'data/en_ewt-up-dev.preprocessed.csv')\n",
        "test_data = read_data_as_sentence('data/en_ewt-up-test.conllu', 'data/en_ewt-up-test.preprocessed.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycGwfyhPkUof"
      },
      "source": [
        "The `read_data_as_sentence()` function returns DataFrames, where each row represents a sentence from the dataset passed to the function. Each sentence has been expanded based on its predicates, resulting in multiple copies of the same sentence, each focused on a different predicate.\n",
        "\n",
        "The DataFrame has two columns:\n",
        "\n",
        "- `input_form`: a list of strings, where each string represents a words in the sentence, followed by two special tokens:\n",
        "    1. A special token (`[SEP]`), which denotes the separation between the words of the sentence and the predicate form. \n",
        "    2. The predicate form, which corresponds to the `argument` values for the same row in the DataFrame.\n",
        "- `argument`: a list of strings, representing the arguments associated with each word in the sentence. The length of each list is equal to the number of words in the sentence, plus two additional elements, for the special token and predicate form. The arguments match the predicate appended to the `input_form` for the same row in the DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explore the DataFrame\n",
        "\n",
        "Before you continue to tokenize the sentences and fine-tune the BERT model, it's time to get more familiar with our data.\n",
        "\n",
        "To explore the DataFrame, start by printing the head of the preprocessed DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4799 entries, 0 to 4798\n",
            "Data columns (total 2 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   input_form  4799 non-null   object\n",
            " 1   argument    4799 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 75.1+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(test_data.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1Fki50OkUog"
      },
      "source": [
        "The **Non-Null** count for both columns should match, indicating there are as many lists of `input_form` values as there are lists of `argument` values, namely one for each sentence.\n",
        "\n",
        "Next, print the words and their argument labels for the first 20 sentences of the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7H8QTMlHkUoh",
        "outputId": "5a58240c-0137-4da2-cb6e-845d1fe3b54c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "form: What            argument: _\n",
            "form: if              argument: _\n",
            "form: Google          argument: ARG1\n",
            "form: Morphed         argument: _\n",
            "form: Into            argument: _\n",
            "form: GoogleOS        argument: ARG2\n",
            "form: ?               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: Morphed         argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: What            argument: _\n",
            "form: if              argument: _\n",
            "form: Google          argument: ARG0\n",
            "form: expanded        argument: _\n",
            "form: on              argument: _\n",
            "form: its             argument: _\n",
            "form: search          argument: _\n",
            "form: -               argument: _\n",
            "form: engine          argument: _\n",
            "form: (               argument: _\n",
            "form: and             argument: _\n",
            "form: now             argument: _\n",
            "form: e-mail          argument: _\n",
            "form: )               argument: _\n",
            "form: wares           argument: ARG1\n",
            "form: into            argument: _\n",
            "form: a               argument: _\n",
            "form: full            argument: _\n",
            "form: -               argument: _\n",
            "form: fledged         argument: _\n",
            "form: operating       argument: _\n",
            "form: system          argument: ARG4\n",
            "form: ?               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: expanded        argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: (               argument: _\n",
            "form: And             argument: _\n",
            "form: ,               argument: _\n",
            "form: by              argument: _\n",
            "form: the             argument: _\n",
            "form: way             argument: _\n",
            "form: ,               argument: _\n",
            "form: is              argument: _\n",
            "form: anybody         argument: _\n",
            "form: else            argument: _\n",
            "form: just            argument: _\n",
            "form: a               argument: _\n",
            "form: little          argument: _\n",
            "form: nostalgic       argument: _\n",
            "form: for             argument: _\n",
            "form: the             argument: _\n",
            "form: days            argument: _\n",
            "form: when            argument: _\n",
            "form: that            argument: _\n",
            "form: was             argument: _\n",
            "form: a               argument: _\n",
            "form: good            argument: _\n",
            "form: thing           argument: _\n",
            "form: ?               argument: _\n",
            "form: )               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: way             argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: (               argument: _\n",
            "form: And             argument: _\n",
            "form: ,               argument: _\n",
            "form: by              argument: _\n",
            "form: the             argument: _\n",
            "form: way             argument: ARGM-DIS\n",
            "form: ,               argument: _\n",
            "form: is              argument: _\n",
            "form: anybody         argument: ARG1\n",
            "form: else            argument: _\n",
            "form: just            argument: _\n",
            "form: a               argument: _\n",
            "form: little          argument: _\n",
            "form: nostalgic       argument: ARG2\n",
            "form: for             argument: _\n",
            "form: the             argument: _\n",
            "form: days            argument: _\n",
            "form: when            argument: _\n",
            "form: that            argument: _\n",
            "form: was             argument: _\n",
            "form: a               argument: _\n",
            "form: good            argument: _\n",
            "form: thing           argument: _\n",
            "form: ?               argument: _\n",
            "form: )               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: is              argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: (               argument: _\n",
            "form: And             argument: _\n",
            "form: ,               argument: _\n",
            "form: by              argument: _\n",
            "form: the             argument: _\n",
            "form: way             argument: _\n",
            "form: ,               argument: _\n",
            "form: is              argument: _\n",
            "form: anybody         argument: _\n",
            "form: else            argument: _\n",
            "form: just            argument: _\n",
            "form: a               argument: _\n",
            "form: little          argument: _\n",
            "form: nostalgic       argument: _\n",
            "form: for             argument: _\n",
            "form: the             argument: _\n",
            "form: days            argument: ARGM-TMP\n",
            "form: when            argument: R-ARGM-TMP\n",
            "form: that            argument: ARG1\n",
            "form: was             argument: _\n",
            "form: a               argument: _\n",
            "form: good            argument: _\n",
            "form: thing           argument: ARG2\n",
            "form: ?               argument: _\n",
            "form: )               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: is              argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: This            argument: _\n",
            "form: BuzzMachine     argument: ARG2\n",
            "form: post            argument: _\n",
            "form: argues          argument: _\n",
            "form: that            argument: _\n",
            "form: Google          argument: _\n",
            "form: 's              argument: _\n",
            "form: rush            argument: _\n",
            "form: toward          argument: _\n",
            "form: ubiquity        argument: _\n",
            "form: might           argument: _\n",
            "form: backfire        argument: _\n",
            "form: --              argument: _\n",
            "form: which           argument: _\n",
            "form: we              argument: _\n",
            "form: 've             argument: _\n",
            "form: all             argument: _\n",
            "form: heard           argument: _\n",
            "form: before          argument: _\n",
            "form: ,               argument: _\n",
            "form: but             argument: _\n",
            "form: it              argument: _\n",
            "form: 's              argument: _\n",
            "form: particularly    argument: _\n",
            "form: well            argument: _\n",
            "form: -               argument: _\n",
            "form: put             argument: _\n",
            "form: in              argument: _\n",
            "form: this            argument: _\n",
            "form: post            argument: _\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: post            argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: This            argument: _\n",
            "form: BuzzMachine     argument: _\n",
            "form: post            argument: ARG0\n",
            "form: argues          argument: _\n",
            "form: that            argument: _\n",
            "form: Google          argument: _\n",
            "form: 's              argument: _\n",
            "form: rush            argument: _\n",
            "form: toward          argument: _\n",
            "form: ubiquity        argument: _\n",
            "form: might           argument: _\n",
            "form: backfire        argument: ARG1\n",
            "form: --              argument: _\n",
            "form: which           argument: _\n",
            "form: we              argument: _\n",
            "form: 've             argument: _\n",
            "form: all             argument: _\n",
            "form: heard           argument: _\n",
            "form: before          argument: _\n",
            "form: ,               argument: _\n",
            "form: but             argument: _\n",
            "form: it              argument: _\n",
            "form: 's              argument: _\n",
            "form: particularly    argument: _\n",
            "form: well            argument: _\n",
            "form: -               argument: _\n",
            "form: put             argument: _\n",
            "form: in              argument: _\n",
            "form: this            argument: _\n",
            "form: post            argument: _\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: argues          argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: This            argument: _\n",
            "form: BuzzMachine     argument: _\n",
            "form: post            argument: _\n",
            "form: argues          argument: _\n",
            "form: that            argument: _\n",
            "form: Google          argument: ARG1\n",
            "form: 's              argument: _\n",
            "form: rush            argument: _\n",
            "form: toward          argument: _\n",
            "form: ubiquity        argument: ARG2\n",
            "form: might           argument: _\n",
            "form: backfire        argument: _\n",
            "form: --              argument: _\n",
            "form: which           argument: _\n",
            "form: we              argument: _\n",
            "form: 've             argument: _\n",
            "form: all             argument: _\n",
            "form: heard           argument: _\n",
            "form: before          argument: _\n",
            "form: ,               argument: _\n",
            "form: but             argument: _\n",
            "form: it              argument: _\n",
            "form: 's              argument: _\n",
            "form: particularly    argument: _\n",
            "form: well            argument: _\n",
            "form: -               argument: _\n",
            "form: put             argument: _\n",
            "form: in              argument: _\n",
            "form: this            argument: _\n",
            "form: post            argument: _\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: rush            argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: This            argument: _\n",
            "form: BuzzMachine     argument: _\n",
            "form: post            argument: _\n",
            "form: argues          argument: _\n",
            "form: that            argument: _\n",
            "form: Google          argument: _\n",
            "form: 's              argument: _\n",
            "form: rush            argument: ARG1\n",
            "form: toward          argument: _\n",
            "form: ubiquity        argument: _\n",
            "form: might           argument: ARGM-MOD\n",
            "form: backfire        argument: _\n",
            "form: --              argument: _\n",
            "form: which           argument: _\n",
            "form: we              argument: _\n",
            "form: 've             argument: _\n",
            "form: all             argument: _\n",
            "form: heard           argument: ARGM-ADV\n",
            "form: before          argument: _\n",
            "form: ,               argument: _\n",
            "form: but             argument: _\n",
            "form: it              argument: _\n",
            "form: 's              argument: _\n",
            "form: particularly    argument: _\n",
            "form: well            argument: _\n",
            "form: -               argument: _\n",
            "form: put             argument: _\n",
            "form: in              argument: _\n",
            "form: this            argument: _\n",
            "form: post            argument: _\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: backfire        argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: This            argument: _\n",
            "form: BuzzMachine     argument: _\n",
            "form: post            argument: _\n",
            "form: argues          argument: _\n",
            "form: that            argument: _\n",
            "form: Google          argument: _\n",
            "form: 's              argument: _\n",
            "form: rush            argument: _\n",
            "form: toward          argument: _\n",
            "form: ubiquity        argument: _\n",
            "form: might           argument: _\n",
            "form: backfire        argument: _\n",
            "form: --              argument: _\n",
            "form: which           argument: _\n",
            "form: we              argument: _\n",
            "form: 've             argument: _\n",
            "form: all             argument: _\n",
            "form: heard           argument: _\n",
            "form: before          argument: _\n",
            "form: ,               argument: _\n",
            "form: but             argument: _\n",
            "form: it              argument: _\n",
            "form: 's              argument: _\n",
            "form: particularly    argument: _\n",
            "form: well            argument: _\n",
            "form: -               argument: _\n",
            "form: put             argument: _\n",
            "form: in              argument: _\n",
            "form: this            argument: _\n",
            "form: post            argument: _\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: 've             argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: This            argument: _\n",
            "form: BuzzMachine     argument: _\n",
            "form: post            argument: _\n",
            "form: argues          argument: _\n",
            "form: that            argument: _\n",
            "form: Google          argument: _\n",
            "form: 's              argument: _\n",
            "form: rush            argument: _\n",
            "form: toward          argument: _\n",
            "form: ubiquity        argument: _\n",
            "form: might           argument: _\n",
            "form: backfire        argument: _\n",
            "form: --              argument: _\n",
            "form: which           argument: ARG1\n",
            "form: we              argument: ARG0\n",
            "form: 've             argument: _\n",
            "form: all             argument: ARGM-ADV\n",
            "form: heard           argument: _\n",
            "form: before          argument: ARGM-TMP\n",
            "form: ,               argument: _\n",
            "form: but             argument: _\n",
            "form: it              argument: _\n",
            "form: 's              argument: _\n",
            "form: particularly    argument: _\n",
            "form: well            argument: _\n",
            "form: -               argument: _\n",
            "form: put             argument: _\n",
            "form: in              argument: _\n",
            "form: this            argument: _\n",
            "form: post            argument: _\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: heard           argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: This            argument: _\n",
            "form: BuzzMachine     argument: _\n",
            "form: post            argument: _\n",
            "form: argues          argument: _\n",
            "form: that            argument: _\n",
            "form: Google          argument: _\n",
            "form: 's              argument: _\n",
            "form: rush            argument: _\n",
            "form: toward          argument: _\n",
            "form: ubiquity        argument: _\n",
            "form: might           argument: _\n",
            "form: backfire        argument: _\n",
            "form: --              argument: _\n",
            "form: which           argument: _\n",
            "form: we              argument: _\n",
            "form: 've             argument: _\n",
            "form: all             argument: _\n",
            "form: heard           argument: _\n",
            "form: before          argument: _\n",
            "form: ,               argument: _\n",
            "form: but             argument: _\n",
            "form: it              argument: ARG1\n",
            "form: 's              argument: _\n",
            "form: particularly    argument: ARGM-ADV\n",
            "form: well            argument: _\n",
            "form: -               argument: _\n",
            "form: put             argument: ARG2\n",
            "form: in              argument: _\n",
            "form: this            argument: _\n",
            "form: post            argument: ARGM-LOC\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: 's              argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: This            argument: _\n",
            "form: BuzzMachine     argument: _\n",
            "form: post            argument: _\n",
            "form: argues          argument: _\n",
            "form: that            argument: _\n",
            "form: Google          argument: _\n",
            "form: 's              argument: _\n",
            "form: rush            argument: _\n",
            "form: toward          argument: _\n",
            "form: ubiquity        argument: _\n",
            "form: might           argument: _\n",
            "form: backfire        argument: _\n",
            "form: --              argument: _\n",
            "form: which           argument: _\n",
            "form: we              argument: _\n",
            "form: 've             argument: _\n",
            "form: all             argument: _\n",
            "form: heard           argument: _\n",
            "form: before          argument: _\n",
            "form: ,               argument: _\n",
            "form: but             argument: _\n",
            "form: it              argument: _\n",
            "form: 's              argument: _\n",
            "form: particularly    argument: _\n",
            "form: well            argument: _\n",
            "form: -               argument: _\n",
            "form: put             argument: _\n",
            "form: in              argument: _\n",
            "form: this            argument: _\n",
            "form: post            argument: _\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: post            argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: Google          argument: ARG1\n",
            "form: is              argument: _\n",
            "form: a               argument: _\n",
            "form: nice            argument: _\n",
            "form: search          argument: _\n",
            "form: engine          argument: ARG2\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: is              argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: Does            argument: _\n",
            "form: anybody         argument: _\n",
            "form: use             argument: _\n",
            "form: it              argument: _\n",
            "form: for             argument: _\n",
            "form: anything        argument: _\n",
            "form: else            argument: _\n",
            "form: ?               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: Does            argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: Does            argument: _\n",
            "form: anybody         argument: ARG0\n",
            "form: use             argument: _\n",
            "form: it              argument: ARG1\n",
            "form: for             argument: _\n",
            "form: anything        argument: ARG2\n",
            "form: else            argument: _\n",
            "form: ?               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: use             argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: They            argument: ARG0\n",
            "form: own             argument: _\n",
            "form: blogger         argument: ARG1\n",
            "form: ,               argument: _\n",
            "form: of              argument: ARGM-ADV\n",
            "form: course          argument: _\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: own             argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: Is              argument: _\n",
            "form: that            argument: ARG1\n",
            "form: a               argument: _\n",
            "form: money           argument: _\n",
            "form: maker           argument: ARG2\n",
            "form: ?               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: Is              argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: I               argument: _\n",
            "form: 'm              argument: _\n",
            "form: staying         argument: _\n",
            "form: away            argument: _\n",
            "form: from            argument: _\n",
            "form: the             argument: _\n",
            "form: stock           argument: _\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: 'm              argument: None\n",
            "\n",
            "========================================\n",
            "\n",
            "form: I               argument: ARG1\n",
            "form: 'm              argument: _\n",
            "form: staying         argument: _\n",
            "form: away            argument: ARG3\n",
            "form: from            argument: _\n",
            "form: the             argument: _\n",
            "form: stock           argument: _\n",
            "form: .               argument: _\n",
            "----------------------------------------\n",
            "form: [SEP]           argument: None\n",
            "form: staying         argument: None\n",
            "\n",
            "========================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_sentences(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, the sequence of word forms runs parallel to the sequence of argument labels. This means that for every index of `input_form`, the same index of `argument` gives its argument label.\n",
        "\n",
        "Argument labels are:\n",
        "- **'_'** for tokens that are not an argument (in the current predicate sense of the sentence).\n",
        "- The token's respective Propbank label for tokens that are an argument, e.g. **ARG1**\n",
        "- **None** for the special separator token (`[SEP]`) and the predicate token that follows the separator.\n",
        "\n",
        "For example, in the the first sentence of the test data printed above (\"What if Google Morphed Into GoogleOS?\"), the predicate 'Morphed' evokes [the frame `morph.01`](https://propbank.github.io/v3.4.0/frames/morph.html#morph.01). The frame's arguments are:\n",
        "\n",
        "- `ARG0-PAG`: causer of transformation\n",
        "- `ARG1-PPT`: thing changing\n",
        "- `ARG2-PRD`: end state\n",
        "- `ARG3-VSP`: start state\n",
        "\n",
        "In this example, the `ARG1` label is assigned to 'Google', and the `ARG2` label is assigned to 'GoogleOS', which indicates 'Google' is the thing that is changing and 'GoogleOS' is its end state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjaHOoSckUoh"
      },
      "source": [
        "## Step 2: Tokenize the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that you have extracted sentences and labels from the datasets, you need to prepare the sentences for the BERT model by tokenizing them.\n",
        "\n",
        "Use HuggingFace's [`AutoTokenizer`](https://huggingface.co/docs/transformers/v4.38.2/en/model_doc/auto#transformers.AutoTokenizer) to construct a DistilBERT tokenizer, which is based on the WordPiece algorithm. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "pGoBoaA8kUoh"
      },
      "outputs": [],
      "source": [
        "# Set the model ID to use\n",
        "model_id = \"distilbert-base-uncased\"\n",
        "\n",
        "# Initialize the tokenizer \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Check the assertion that the tokenizer is an instance of transformers.PreTrainedTokenizerFast\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acI7hd8KkUoi"
      },
      "source": [
        "To test the `tokenizer()`, tokenize the first sentence of the test data, including:\n",
        "\n",
        "- `add_special_tokens` set to **True** to add a `[CLS]` token to the start of every sentence.\n",
        "- `is_split_into_words` set to **True** because the sentence is already split into words (based on the Universal Propbank 1.0 dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Wg7_MEckUoi",
        "outputId": "c4457c76-b7e0-4299-b4c5-3a71274fe3cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     [CLS] 101\n",
            "      what 2054\n",
            "        if 2065\n",
            "    google 8224\n",
            "       mor 22822\n",
            "      ##ph 8458\n",
            "      ##ed 2098\n",
            "      into 2046\n",
            "    google 8224\n",
            "      ##os 2891\n",
            "         ? 1029\n",
            "     [SEP] 102\n",
            "       mor 22822\n",
            "      ##ph 8458\n",
            "      ##ed 2098\n",
            "     [SEP] 102\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the first example in the test data\n",
        "example = test_data['input_form'][0]\n",
        "tokenized_input = tokenizer(example,add_special_tokens=True, is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "\n",
        "# Print the example tokens and their corresponding IDs\n",
        "for token, id in zip(tokens, tokenized_input[\"input_ids\"]):\n",
        "    print(f\"{token:>10} {id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You've successfully tokenized the sample sentence, splitting words up into subword tokens and fetching their token IDs from DistilBERT's vocabulary.\n",
        "\n",
        "> Note: notice how the special tokens `[CLS]` and `[SEP]` are tokenized as **101** and **102**. These numbers are meaningful to BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJcd5rKIkUoi"
      },
      "source": [
        "## Step 3: Prepare the input for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ergy_gI8kUoi"
      },
      "source": [
        "Before training the model, map the labels in the datasets to numerical values. This ensures consistency and facilitates the training process.\n",
        "\n",
        "To get the label mapping, call `get_label_mapping()`, including:\n",
        "\n",
        "| Parameter name     | Required | Parameter description |\n",
        "|--------------------|:--------------:|-------------|\n",
        "| *positional 1* (DataFrame)          | ✅️ | The training dataset for which to extract the label mapping. |\n",
        "| *positional 2* (DataFrame)          | ✅ | The test dataset for which to extract the label mapping.|\n",
        "| *positional 3* (DataFrame)          | ✅ | The dev dataset for which to extract the label mapping. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "01kBZUaZkUoi"
      },
      "outputs": [],
      "source": [
        "label_map = get_label_mapping(train_data, test_data, dev_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `get_label_mapping()` function returns an alphabetically-ordered dictionary mapping:\n",
        "- **_** to **0**.\n",
        "- String labels to integers, e.g. **ARG0** to **1**.\n",
        "- **None** to **None**, to preserve the labels for special tokens and predicates. (You will replace **None** with **-100** later to mask these tokens from being labeled.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NG6-s0DkUoi",
        "outputId": "100c7619-30ec-4b19-a7e0-c03e5857a487"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'_': 0, 'ARG0': 1, 'ARG1': 2, 'ARG1-DSP': 3, 'ARG2': 4, 'ARG3': 5, 'ARG4': 6, 'ARG5': 7, 'ARGA': 8, 'ARGM-ADJ': 9, 'ARGM-ADV': 10, 'ARGM-CAU': 11, 'ARGM-COM': 12, 'ARGM-CXN': 13, 'ARGM-DIR': 14, 'ARGM-DIS': 15, 'ARGM-EXT': 16, 'ARGM-GOL': 17, 'ARGM-LOC': 18, 'ARGM-LVB': 19, 'ARGM-MNR': 20, 'ARGM-MOD': 21, 'ARGM-NEG': 22, 'ARGM-PRD': 23, 'ARGM-PRP': 24, 'ARGM-PRR': 25, 'ARGM-REC': 26, 'ARGM-TMP': 27, 'C-ARG0': 28, 'C-ARG1': 29, 'C-ARG1-DSP': 30, 'C-ARG2': 31, 'C-ARG3': 32, 'C-ARG4': 33, 'C-ARGM-ADV': 34, 'C-ARGM-COM': 35, 'C-ARGM-CXN': 36, 'C-ARGM-DIR': 37, 'C-ARGM-EXT': 38, 'C-ARGM-GOL': 39, 'C-ARGM-LOC': 40, 'C-ARGM-MNR': 41, 'C-ARGM-PRP': 42, 'C-ARGM-PRR': 43, 'C-ARGM-TMP': 44, 'R-ARG0': 45, 'R-ARG1': 46, 'R-ARG2': 47, 'R-ARG3': 48, 'R-ARG4': 49, 'R-ARGM-ADJ': 50, 'R-ARGM-ADV': 51, 'R-ARGM-CAU': 52, 'R-ARGM-COM': 53, 'R-ARGM-DIR': 54, 'R-ARGM-GOL': 55, 'R-ARGM-LOC': 56, 'R-ARGM-MNR': 57, 'R-ARGM-TMP': 58, None: None}\n"
          ]
        }
      ],
      "source": [
        "print(label_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnmI8mDBkUoi"
      },
      "source": [
        "\n",
        "Next, apply the label mapping to the datasets, adding the column `mapped_labels` to the DataFrames. This column contains arrays of integers representing the labels, based on the label mapping. \n",
        "\n",
        "To apply the label mapping, call `map_labels_in_dataframe()`, including:\n",
        "\n",
        "| Parameter name     | Required | Parameter description |\n",
        "|--------------------|:--------------:|-------------|\n",
        "| *positional 1*                   | ✅️ | The DataFrame for which to convert the argument labels. |\n",
        "| *positional 2*                 | ✅ | The label mapping, created with `get_label_mapping()`. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "KCuAQ_l0kUoj"
      },
      "outputs": [],
      "source": [
        "train_data = map_labels_in_dataframe(train_data, label_map)\n",
        "dev_data = map_labels_in_dataframe(dev_data, label_map)\n",
        "test_data = map_labels_in_dataframe(test_data, label_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyXubrzukUoj"
      },
      "source": [
        "As you can see, for each row in the DataFrame, the values in `mapped_labels` and `arguments` correspond to the mapping in `label_map`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1Kdo09m2kUoj",
        "outputId": "214847c9-36ba-400f-b91b-58a258553788"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input_form</th>\n",
              "      <th>argument</th>\n",
              "      <th>mapped_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[What, if, Google, Morphed, Into, GoogleOS, ?,...</td>\n",
              "      <td>[_, _, ARG1, _, _, ARG2, _, None, None]</td>\n",
              "      <td>[0, 0, 2, 0, 0, 4, 0, None, None]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[What, if, Google, expanded, on, its, search, ...</td>\n",
              "      <td>[_, _, ARG0, _, _, _, _, _, _, _, _, _, _, _, ...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[(, And, ,, by, the, way, ,, is, anybody, else...</td>\n",
              "      <td>[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[(, And, ,, by, the, way, ,, is, anybody, else...</td>\n",
              "      <td>[_, _, _, _, _, ARGM-DIS, _, _, ARG1, _, _, _,...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 15, 0, 0, 2, 0, 0, 0, 0, 4, 0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[(, And, ,, by, the, way, ,, is, anybody, else...</td>\n",
              "      <td>[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          input_form  \\\n",
              "0  [What, if, Google, Morphed, Into, GoogleOS, ?,...   \n",
              "1  [What, if, Google, expanded, on, its, search, ...   \n",
              "2  [(, And, ,, by, the, way, ,, is, anybody, else...   \n",
              "3  [(, And, ,, by, the, way, ,, is, anybody, else...   \n",
              "4  [(, And, ,, by, the, way, ,, is, anybody, else...   \n",
              "\n",
              "                                            argument  \\\n",
              "0            [_, _, ARG1, _, _, ARG2, _, None, None]   \n",
              "1  [_, _, ARG0, _, _, _, _, _, _, _, _, _, _, _, ...   \n",
              "2  [_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, ...   \n",
              "3  [_, _, _, _, _, ARGM-DIS, _, _, ARG1, _, _, _,...   \n",
              "4  [_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, ...   \n",
              "\n",
              "                                       mapped_labels  \n",
              "0                  [0, 0, 2, 0, 0, 4, 0, None, None]  \n",
              "1  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...  \n",
              "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "3  [0, 0, 0, 0, 0, 15, 0, 0, 2, 0, 0, 0, 0, 4, 0,...  \n",
              "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLQrg5KhkUoj"
      },
      "source": [
        "Now that you have initialized and tested the `tokenizer()` and added mapped labels to the DataFrames, it's time to tokenize (and pad) all sentences. \n",
        "\n",
        "Since WordPiece tokenization potentially breaks words up into subword tokens, the tokens and their labels have to be re-aligned. The `tokenize_and_align_labels()` function you'll call for this iterates over each token and determines the appropriate label based on the provided dataset. \n",
        "\n",
        "Special tokens are assigned a label of **-100** to indicate they should be ignored in the loss function. Labels for the first token of each word are set accordingly, while labels for subsequent tokens within the same word are determined based on the `label_all_tokens` flag.\n",
        "\n",
        "To tokenize the sentences and align the labels, call `tokenize_and_align_labels()`, including:\n",
        "\n",
        "| Parameter name     | Required | Parameter description |\n",
        "|--------------------|:--------------:|-------------|\n",
        "| *positional 1* (`transformers AutoTokenizer`) | ✅️ | The `tokenizer()` for the pre-trained model. |\n",
        "| *positional 2* (DataFrame) | ✅ | The preprocessed datasets |\n",
        "| `label_all_tokens` (boolean)     | Optional (defaults to **True**) | Whether all tokens should receive their own label, accounting for words split into subtokens |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "zswrLWMWkUoj"
      },
      "outputs": [],
      "source": [
        "tokenized_test = tokenize_and_align_labels(tokenizer, test_data, label_all_tokens=True)\n",
        "tokenized_train = tokenize_and_align_labels(tokenizer, train_data, label_all_tokens=True)\n",
        "tokenized_dev = tokenize_and_align_labels(tokenizer, dev_data, label_all_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilTVwpHikUoj"
      },
      "source": [
        "Now that you have tokenized all three datasets, let's examine the result. \n",
        "\n",
        "The `tokenized_` datasets are of the type `transformers.tokenization_utils_base.BatchEncoding` and have three attributes per row:\n",
        "\n",
        "1. `input_ids`: an array of token IDs for the tokenized sentence. Starts with the token ID for the `[CLS]` token, followed by the tokenized sentence, the `[SEP]` token, the predicate, and a final `[SEP]` token. \n",
        "2. `attention_mask`: an array representing the attention mask for the sentence.\n",
        "3. `labels`: an array with numerical labels, aligned with the tokens. \n",
        "\n",
        "> Note: all three arrays are padded so that every sample per dataset is of equal length.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHZ-bA_YkUoj",
        "outputId": "b213bb1f-7bdb-4aa3-8c33-f404e8f4e808"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
            "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
            "['[CLS]', 'what', 'if', 'google', 'mor', '##ph', '##ed', 'into', 'google', '##os', '?', '[SEP]', 'mor', '##ph', '##ed', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "input_ids: tensor([  101,  2054,  2065,  8224, 22822,  8458,  2098,  2046,  8224,  2891,\n",
            "         1029,   102, 22822,  8458,  2098,   102,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0])\n",
            "attention_mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0])\n",
            "labels: [-100, 0, 0, 2, 0, 0, 0, 0, 4, 4, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"
          ]
        }
      ],
      "source": [
        "print(type(tokenized_test))\n",
        "print(tokenized_test.keys())\n",
        "print(tokenizer.convert_ids_to_tokens(tokenized_test[\"input_ids\"][0]))\n",
        "for key in tokenized_test.keys():\n",
        "    print(f\"{key}: {tokenized_test[key][0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sE82NbRkUoj"
      },
      "source": [
        "To confirm that we have padded all sentences in the `tokenized_test` dataset to be of equal length, let's check the length of all three arrays for the first 10 sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRDQ0HDFkUoj",
        "outputId": "e5659bed-af73-4244-8fcd-f48994989f79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentence 0: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
            "sentence 1: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
            "sentence 2: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
            "sentence 3: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
            "sentence 4: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
            "sentence 5: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
            "sentence 6: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
            "sentence 7: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
            "sentence 8: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n",
            "sentence 9: input_ids: 97 \tlabels: 97 \tattention_mask: 97\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    print(f\"sentence {i}:\", \"input_ids:\", len(tokenized_test[\"input_ids\"][i]), \"\\tlabels:\", len(tokenized_test[\"labels\"][i]), \"\\tattention_mask:\", len(tokenized_test[\"attention_mask\"][i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CUO9JB9kUoj"
      },
      "source": [
        "Converting the tokenized data to datasets format with the function `load_dataset`\n",
        "\n",
        "Now that you have tokenized and padded the sentences, and aligned the labels with the tokens, you're ready to transform the tokenized datasets into Hugging Face's [`datasets.arrow_dataset.Dataset`](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset).\n",
        "\n",
        "To transform the tokenized datasets into `Dataset` objects, call the `load_dataset()` function, which calls the [`Dataset.from_dict()` method](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.from_dict), including:\n",
        "\n",
        "| Parameter name     | Required | Parameter description |\n",
        "|--------------------|:--------------:|-------------|\n",
        "| *positional 1* (`transformers.tokenization_utils_base.BatchEncoding`) | ✅️ | The tokenized dataset. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "A0qvLaa1kUoj"
      },
      "outputs": [],
      "source": [
        "dataset_train = load_dataset(tokenized_train)\n",
        "dataset_dev = load_dataset(tokenized_dev)\n",
        "dataset_test = load_dataset(tokenized_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's print the type of the resulting dataset, to confirm the transformation into `datasets.arrow_dataset.Dataset`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'datasets.arrow_dataset.Dataset'>\n"
          ]
        }
      ],
      "source": [
        "print(type(dataset_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg-Ku9KCkUok"
      },
      "source": [
        "## Step 4: Fine-tune the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OiJgg86kUok"
      },
      "source": [
        "Finally, the sentences have been transformed from CoNNL-U Plus format to Hugging Face `Dataset` objects: it's time to fine-tune BERT!\n",
        "\n",
        "Fine-tuning a BERT model on the full dataset can be a very computationally challenging task. To speed up the process, create subsets of the three datasets with 1000 samples per dataset, selected randomly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "J1L3vUjBkUok"
      },
      "outputs": [],
      "source": [
        "small_train_dataset = dataset_train.shuffle(seed=42).select(range(1000))\n",
        "small_eval_dataset = dataset_dev.shuffle(seed=42).select(range(1000))\n",
        "small_test_dataset = dataset_test.shuffle(seed=42).select(range(1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0tUEUrrkUok"
      },
      "source": [
        "To map the numerical labels back to their string representations, you need to convert the `label_map` dictionary to a list of labels (as strings).\n",
        "\n",
        "To convert the `label_map` to a list of labels (as strings), call the `get_labels_from_map()` function, including:\n",
        "\n",
        "| Parameter name     | Required | Parameter description |\n",
        "|--------------------|:--------------:|-------------|\n",
        "| *positional 1* (dictionary) | ✅️ | The dictionary mapping labels as strings to their numerical represenation. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "zeczaSQlkUok"
      },
      "outputs": [],
      "source": [
        "label_list = get_labels_from_map(label_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPthsO7KkUoq"
      },
      "source": [
        "Next, load the pretrained DistilBERT model using the [`AutoModelForTokenClassification.from_pretrained()` method](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained) from the `transformers` library, together with the model name (**distilbert-base-uncased**), and the [`TrainingArguments`](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/trainer#transformers.TrainingArguments) neccesary for training.\n",
        "\n",
        "To get the model, model name and `TrainingArguments`, call the `load_srl_model()` function, including:\n",
        "\n",
        "| Parameter name     | Required | Parameter description |\n",
        "|--------------------|:--------------:|-------------|\n",
        "| *positional 1* (string) | ✅️ | The model identifier.  |\n",
        "| *positional 2* (list of strings) | ✅️ | The tokenized dataset. |\n",
        "| `batch_size` (integer) | Optional (defaults to **16**) | The [batch size for training](https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#methods-and-tools-for-efficient-training-on-a-single-gpu) and [inference](https://huggingface.co/docs/setfit/main/en/how_to/batch_sizes). |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOKg-GaWkUoq",
        "outputId": "745e5d98-6ef6-44ec-ece8-9c0ed6d34f9e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /Users/kris/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\",\n",
            "    \"14\": \"LABEL_14\",\n",
            "    \"15\": \"LABEL_15\",\n",
            "    \"16\": \"LABEL_16\",\n",
            "    \"17\": \"LABEL_17\",\n",
            "    \"18\": \"LABEL_18\",\n",
            "    \"19\": \"LABEL_19\",\n",
            "    \"20\": \"LABEL_20\",\n",
            "    \"21\": \"LABEL_21\",\n",
            "    \"22\": \"LABEL_22\",\n",
            "    \"23\": \"LABEL_23\",\n",
            "    \"24\": \"LABEL_24\",\n",
            "    \"25\": \"LABEL_25\",\n",
            "    \"26\": \"LABEL_26\",\n",
            "    \"27\": \"LABEL_27\",\n",
            "    \"28\": \"LABEL_28\",\n",
            "    \"29\": \"LABEL_29\",\n",
            "    \"30\": \"LABEL_30\",\n",
            "    \"31\": \"LABEL_31\",\n",
            "    \"32\": \"LABEL_32\",\n",
            "    \"33\": \"LABEL_33\",\n",
            "    \"34\": \"LABEL_34\",\n",
            "    \"35\": \"LABEL_35\",\n",
            "    \"36\": \"LABEL_36\",\n",
            "    \"37\": \"LABEL_37\",\n",
            "    \"38\": \"LABEL_38\",\n",
            "    \"39\": \"LABEL_39\",\n",
            "    \"40\": \"LABEL_40\",\n",
            "    \"41\": \"LABEL_41\",\n",
            "    \"42\": \"LABEL_42\",\n",
            "    \"43\": \"LABEL_43\",\n",
            "    \"44\": \"LABEL_44\",\n",
            "    \"45\": \"LABEL_45\",\n",
            "    \"46\": \"LABEL_46\",\n",
            "    \"47\": \"LABEL_47\",\n",
            "    \"48\": \"LABEL_48\",\n",
            "    \"49\": \"LABEL_49\",\n",
            "    \"50\": \"LABEL_50\",\n",
            "    \"51\": \"LABEL_51\",\n",
            "    \"52\": \"LABEL_52\",\n",
            "    \"53\": \"LABEL_53\",\n",
            "    \"54\": \"LABEL_54\",\n",
            "    \"55\": \"LABEL_55\",\n",
            "    \"56\": \"LABEL_56\",\n",
            "    \"57\": \"LABEL_57\",\n",
            "    \"58\": \"LABEL_58\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_14\": 14,\n",
            "    \"LABEL_15\": 15,\n",
            "    \"LABEL_16\": 16,\n",
            "    \"LABEL_17\": 17,\n",
            "    \"LABEL_18\": 18,\n",
            "    \"LABEL_19\": 19,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_20\": 20,\n",
            "    \"LABEL_21\": 21,\n",
            "    \"LABEL_22\": 22,\n",
            "    \"LABEL_23\": 23,\n",
            "    \"LABEL_24\": 24,\n",
            "    \"LABEL_25\": 25,\n",
            "    \"LABEL_26\": 26,\n",
            "    \"LABEL_27\": 27,\n",
            "    \"LABEL_28\": 28,\n",
            "    \"LABEL_29\": 29,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_30\": 30,\n",
            "    \"LABEL_31\": 31,\n",
            "    \"LABEL_32\": 32,\n",
            "    \"LABEL_33\": 33,\n",
            "    \"LABEL_34\": 34,\n",
            "    \"LABEL_35\": 35,\n",
            "    \"LABEL_36\": 36,\n",
            "    \"LABEL_37\": 37,\n",
            "    \"LABEL_38\": 38,\n",
            "    \"LABEL_39\": 39,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_40\": 40,\n",
            "    \"LABEL_41\": 41,\n",
            "    \"LABEL_42\": 42,\n",
            "    \"LABEL_43\": 43,\n",
            "    \"LABEL_44\": 44,\n",
            "    \"LABEL_45\": 45,\n",
            "    \"LABEL_46\": 46,\n",
            "    \"LABEL_47\": 47,\n",
            "    \"LABEL_48\": 48,\n",
            "    \"LABEL_49\": 49,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_50\": 50,\n",
            "    \"LABEL_51\": 51,\n",
            "    \"LABEL_52\": 52,\n",
            "    \"LABEL_53\": 53,\n",
            "    \"LABEL_54\": 54,\n",
            "    \"LABEL_55\": 55,\n",
            "    \"LABEL_56\": 56,\n",
            "    \"LABEL_57\": 57,\n",
            "    \"LABEL_58\": 58,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.16.0\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /Users/kris/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "model, model_name, args = load_srl_model(model_id, label_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXNIkoswkUoq"
      },
      "source": [
        "Load the seqeval metric to compute the metrics from the predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC6m2vTEkUoq",
        "outputId": "fb0b6522-be1e-4871-934b-41e1bf037f0a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/h3/sbz537gn1cx04npcfycz4mgr0000gp/T/ipykernel_22454/152412463.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"seqeval\")\n",
            "/Users/kris/anaconda3/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "metric = load_metric(\"seqeval\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inp5veTjkUoq"
      },
      "source": [
        "metric.compute(predictions=[label_list], references=[label_list])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNdiQl3RkUoq"
      },
      "source": [
        "Passing the arguments along with the datasets to the `trainer` function to fine-tune the model for semantic role labelling with `trainer.train()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k9xwibdUkUoq",
        "outputId": "122d5196-6924-4259-ddd6-1477dcfb21b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kris/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 40482\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 7593\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='139' max='7593' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 139/7593 23:52 < 21:39:07, 0.10 it/s, Epoch 0.05/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[58], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m         model,\n\u001b[1;32m      3\u001b[0m         args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m         compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m p: compute_metrics(\u001b[38;5;241m*\u001b[39mp, label_list, metric)\n\u001b[1;32m      8\u001b[0m     )\n\u001b[0;32m----> 9\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1365\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1363\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1365\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1368\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1370\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1371\u001b[0m ):\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1940\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast_smart_context_manager():\n\u001b[0;32m-> 1940\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   1942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1943\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1972\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1970\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1971\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1972\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m   1973\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   1974\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   1975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:958\u001b[0m, in \u001b[0;36mDistilBertForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    956\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 958\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistilbert(\n\u001b[1;32m    959\u001b[0m     input_ids,\n\u001b[1;32m    960\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    961\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m    962\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    963\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    964\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    965\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    966\u001b[0m )\n\u001b[1;32m    968\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    970\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:549\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    548\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[1;32m    550\u001b[0m     x\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    551\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    552\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m    553\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    554\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    555\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    556\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:327\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    325\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> 327\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    328\u001b[0m     x\u001b[38;5;241m=\u001b[39mhidden_state, attn_mask\u001b[38;5;241m=\u001b[39mattn_mask, head_mask\u001b[38;5;241m=\u001b[39mhead_mask[i], output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions\n\u001b[1;32m    329\u001b[0m )\n\u001b[1;32m    330\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:287\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    284\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa_layer_norm(sa_output \u001b[38;5;241m+\u001b[39m x)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# Feed Forward Network\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m ffn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(sa_output)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    288\u001b[0m ffn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer_norm(ffn_output \u001b[38;5;241m+\u001b[39m sa_output)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    290\u001b[0m output \u001b[38;5;241m=\u001b[39m (ffn_output,)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:238\u001b[0m, in \u001b[0;36mFFN.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m apply_chunking_to_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim, \u001b[38;5;28minput\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:2438\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2435\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m   2436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m-> 2438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forward_fn(\u001b[38;5;241m*\u001b[39minput_tensors)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:241\u001b[0m, in \u001b[0;36mFFN.ff_chunk\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mff_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m--> 241\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin1(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    242\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n\u001b[1;32m    243\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin2(x)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "        model,\n",
        "        args,\n",
        "        train_dataset=dataset_train,\n",
        "        eval_dataset=dataset_dev,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=lambda p: compute_metrics(*p, label_list, metric)\n",
        "    )\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHnFT6e9kUoq"
      },
      "source": [
        "Evaluate a model fine-tuned for semantic role labelling with `trainer.evaluate()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "2TVKW8rokUor",
        "outputId": "eaa16b96-366f-429d-91df-712d0da01531"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GqcKCCYkUor"
      },
      "source": [
        "After training is finished, the precision/recall/f1 for each category can be computed. \\\n",
        "The same function `compute_metrics` is applied on the result of the predict method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "vFCI1xe3kUor",
        "outputId": "1d022d82-3acb-44e2-d875-6b67cee0a649"
      },
      "outputs": [],
      "source": [
        "predictions, labels, _ = trainer.predict(dataset_test)\n",
        "results = compute_metrics(predictions, labels, label_list, metric)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1WMX0JIkUor"
      },
      "source": [
        "Writing the predictions together with the gold labels to a csv file with the function `write_predictions_to_csv` so that the metrics per class can be computed with the `compute_evaluation_metrics_from_csv` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGTNN01wkUor",
        "outputId": "2538512d-2ad9-4eef-eeb4-e1ddb2d5a50d"
      },
      "outputs": [],
      "source": [
        "results_file = \"predictions.csv\"\n",
        "write_predictions_to_csv(predictions, labels, label_list, results_file)\n",
        "f1,classification_report = compute_evaluation_metrics_from_csv(\"predictions.csv\")\n",
        "print(classification_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbOXCSKFFPk2"
      },
      "source": [
        "Then, we save fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaKRHV59kUor"
      },
      "outputs": [],
      "source": [
        "# Use these codes to save model:\n",
        "tokenizer.save_pretrained(\"tokenizer.save_pretrained.distillbert-base-uncased-finetuned-srl\")\n",
        "trainer.save_model(\"trainer.save_model.distillbert-base-uncased-finetuned-srl\")\n",
        "model.save_pretrained(\"model.save_pretrained.distillbert-base-uncased-finetuned-srl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKLvKTjXFUD6"
      },
      "source": [
        "Here, we copy saved model to google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Nb_gxIdp1jN"
      },
      "outputs": [],
      "source": [
        "!cp -r '/content/trainer.save_model.distillbert-base-uncased-finetuned-srl' '/content/drive/MyDrive/NLP_3_baseline_model/model'\n",
        "!cp -r '/content/model.save_pretrained.distillbert-base-uncased-finetuned-srl' '/content/drive/MyDrive/NLP_3_baseline_model/model'\n",
        "!cp -r '/content/tokenizer.save_pretrained.distillbert-base-uncased-finetuned-srl' '/content/drive/MyDrive/NLP_3_baseline_model/model'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
